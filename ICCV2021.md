* 推荐阅读：<br>
  * [CVPR2017~CVPR2021论文最全整理（paper/code/直播）](https://github.com/extreme-assistant/CVPR2021-Paper-Code-Interpretation)
  * [2020-2021年计算机视觉综述论文汇总](https://github.com/extreme-assistant/survey-computer-vision)<br>
  * [国内外优秀的计算机视觉团队汇总](https://github.com/extreme-assistant/Awesome-CV-Team)

------

# ICCV2021最新信息及论文下载贴（Papers/Codes/Project/PaperReading／Demos/直播分享／论文分享会等）

官网链接：http://iccv2021.thecvf.com<br>
时间：2021年10月11日-10月17日<br>
论文接收公布时间：2021年7月22日<br>

相关问题：<br>

* [如何评价ICCV 2021 的论文接收结果？有哪些值得关注的研究？](https://www.zhihu.com/question/473907895)<br>

<br><br>

# 目录

[1. ICCV2021接受论文/代码分方向汇总（更新中）](#1)<br>
[2. ICCV2021 Oral（更新中）](#2)<br>
[3. ICCV2021论文解读汇总（更新中）](#3)<br>




<br><br>

<a name="1"/> 

# 1.ICCV2021接受论文/代码分方向汇总（更新中）

## 分类目录：

### [1. 检测](#detection)

* [2D目标检测(2D Object Detection)](#IOD)
* [视频目标检测(Video Object Detection)](#VOD)
* [3D目标检测(3D Object Detection)](#3DOD)
* [人物交互检测(HOI Detection)](#HOI)
* [伪装目标检测(Camouflaged Object Detection)](#COD)
* [旋转目标检测(Rotation Object Detection)](#ROD)
* [显著性目标检测(Saliency Object Detection)](#SOD)
* [图像异常检测/表面缺陷检测(Anomally Detection in Image)](#ADI)
* [关键点检测(Keypoint Detection)](#KeypointDetection)
* [边缘检测(Edge Detection)](#EdgeDetection)

### [2. 分割(Segmentation)](#Segmentation)

* [图像分割(Image Segmentation)](#ImageSegmentation)
* [全景分割(Panoptic Segmentation)](#PanopticSegmentation)
* [语义分割(Semantic Segmentation)](#SemanticSegmentation)
* [实例分割(Instance Segmentation)](#InstanceSegmentation)
* [超像素(Superpixel)](#Superpixel)
* [视频目标分割(Video Object Segmentation)](#VOS)
* [参考图像分割(Referring Image Segmentation)](#RIS)
* [抠图(Matting)](#Matting)
* [密集预测(Dense Prediction)](#DensePrediction)

### [3. 图像处理(Image Processing)](#ImageProcessing)

* [超分辨率(Super Resolution)](#SuperResolution)
* [图像复原/图像增强(Image Restoration)](#ImageRestoration)
* [图像去阴影/去反射(Image Shadow Removal/Image Reflection Removal)](#ISR)
* [图像去噪/去模糊/去雨去雾(Image Denoising)](#ImageDenoising)
* [图像编辑/修复(Image Edit/Image Inpainting)](#ImageEdit)
* [图像翻译(Image Translation)](#ImageTranslation)
* [图像质量评估(Image Quality Assessment)](#IQA)
* [风格迁移(Style Transfer)](#StyleTransfer)
* [网络视频传输(Neural Video Delivery)](#NeuralVideoDelivery)

### [4. 估计(Estimation)](#Estimation)

* [姿态估计(Pose Estimation)](#HumanPoseEstimation)
* [手势估计(Gesture Estimation)](#GestureEstimation)
* [光流/位姿/运动估计(Flow/Pose/Motion Estimation)](#Flow/Pose/MotionEstimation)
* [深度估计(Depth Estimation)](#DepthEstimation)

### [5. 图像&视频检索/理解(Image&Video Retrieval/Video Understanding)](#ImageRetrieval)

* [行为识别/行为识别/动作识别/检测/分割(Action/Activity Recognition)](#ActionRecognition)
* [行人重识别/检测(Re-Identification/Detection)](#Re-Identification)
* [图像/视频字幕(Image/Video Caption)](#VideoCaption)

### [6. 人脸(Face)](#Face)

* [人脸识别/检测(Facial Recognition/Detection)](#FacialRecognition)
* [人脸生成/合成/重建/编辑(Face Generation/Face Synthesis/Face Reconstruction/Face Editing)](#FaceSynthesis)
* [人脸伪造/反欺骗(Face Forgery/Face Anti-Spoofing)](#FaceAnti-Spoofing)

### [7. 三维视觉(3D Vision)](#3DVision)

* [点云(Point Cloud)](#3DPC)
* [三维重建(3D Reconstruction)](#3DReconstruction)

### [8. 目标跟踪(Object Tracking)](#ObjectTracking)

### [9. 医学影像(Medical Imaging)](#MedicalImaging)

### [10. 文本检测/识别(Text Detection/Recognition)](#TDR)

### [11. 遥感图像(Remote Sensing Image)](#RSI)

### [12. GAN/生成式/对抗式(GAN/Generative/Adversarial)](#GAN)

### [13. 图像生成/合成(Image Generation/Image Synthesis)](#IGIS)

* [视图合成(View Synthesis)](#ViewSynthesis)

### [14. 场景图(Scene Graph)](#SG)

* [场景图生成(Scene Graph Generation)](#SGG)
* [场景图预测(Scene Graph Prediction)](#SGP)
* [场景图理解(Scene Graph Understanding)](#SGU)

### [15. 视觉定位(Visual Localization)](#VisualLocalization)
* [图像匹配(Image Matching)](#ImageMatching)

### [16. 视觉推理/视觉问答(Visual Reasoning/VQA)](#VisualReasoning)

### [17. 图像分类(Image Classification)](#ImageClassification)

### [18. 神经网络设计与优化(Neural Network Design & Optimization)](#NNS)

* [CNN](#CNN)
* [Attention](#Attention)
* [Transformer](#Transformer)
* [图神经网络(GNN)](#GNN)
* [神经网络架构搜索(NAS)](#NAS)
* [损失函数(Loss Function)](#LossFunction)
* [可视化/可解释性(Visualization/Interpretability)](#Visualization)

### [19. 模型压缩(Model Compression)](#ModelCompression)

* [知识蒸馏(Knowledge Distillation)](#KnowledgeDistillation)
* [剪枝(Pruning)](#Pruning)
* [量化(Quantization)](#Quantization)

### [20. 模型训练/泛化/预测(Model Training/Generalization/Prediction)](#ModelTraining)

* [噪声标签(Noisy Label)](#NoisyLabel)
* [长尾分布(Long-Tailed Distribution)](#Long-Tailed)
* [分布外样本(Out of Distribution Examples)](#OOD)

### [21. 模型评估(Model Evaluation)](#ModelEvaluation)

### [22. 数据处理(Data Processing)](#DataProcessing)

* [数据增广(Data Augmentation)](#DataAugmentation)
* [表征学习(Representation Learning)](#RepresentationLearning)
* [归一化/正则化(Batch Normalization)](#BatchNormalization)
* [图像聚类(Image Clustering)](#ImageClustering)
* [图像压缩(Image Compression)](#ImageCompression)
* [异常检测(Anomaly Detection)](#AnomalyDetection)

### [23. 主动学习(Active Learning)](#ActiveLearning)

### [24. 小样本学习/零样本学习(Few-shot/Zero-shot Learning)](#Few-shotLearning)

### [25. 持续学习(Continual Learning/Life-long Learning)](#ContinualLearning)

### [26. 迁移学习/domain/自适应(Transfer Learning/Domain Adaptation)](#domain)

### [27. 度量学习(Metric Learning)](#MetricLearning)

### [28. 对比学习(Contrastive Learning)](#ContrastiveLearning)

### [29. 增量学习(Incremental Learning)](#IncrementalLearning)

### [30. 强化学习(Reinforcement Learning)](#RL)

### [31. 元学习(Meta Learning)](#MetaLearning)

### [32. 多模态学习(Multi-Modal Learning)](#MMLearning)

* [视听学习(Audio-visual Learning)](#Audio-VisualLearning)
* [视觉语言(Visual & Language)](#VisualLanguage)

### [33. 视觉预测(Vision-based Prediction)](#Vision-basedPrediction)

### [34. 数据集(Dataset)](#Dataset)

### [暂无分类](#100)


<br><br>

<a name="detection"/> 

## 检测

<a name="IOD"/> 

### 2D目标检测(2D Object Detection)

[14] FOVEA: Foveated Image Magnification for Autonomous Navigation<br>
[paper](https://arxiv.org/abs/2108.12102) | [project](https://www.cs.cmu.edu/~mengtial/proj/fovea/)<br><br>

[13] DeFRCN: Decoupled Faster R-CNN for Few-Shot Object Detection<br>
[paper](https://arxiv.org/abs/2108.09017)<br><br>

[12] G-DetKD: Towards General Distillation Framework for Object Detectors via Contrastive and Semantic-guided Feature Imitation<br>
[paper](https://arxiv.org/abs/2108.07482)<br><br>

[11] Vector-Decomposed Disentanglement for Domain-Invariant Object Detection<br>
[paper](https://arxiv.org/abs/2108.06685)<br><br>

[10] Oriented R-CNN for Object Detection<br>
[paper](https://arxiv.org/abs/2108.05699) | [code](https://github.com/jbwang1997/OBBDetection)<br><br>

[9] Conditional DETR for Fast Training Convergence<br>
[paper](https://arxiv.org/abs/2108.06152) | [code](https://git.io/ConditionalDETR)<br><br>

[8] Boosting Weakly Supervised Object Detection via Learning Bounding Box Adjusters<br>
[paper](https://arxiv.org/abs/2108.01499) | [code](https://github.com/DongSky/lbba_boosted_wsod)<br><br>

[7] GraphFPN: Graph Feature Pyramid Network for Object Detection<br>
[paper](https://arxiv.org/abs/2108.00580)<br>
[解读：复旦&港大提出GraphFPN：用图特征金字塔提升目标检测性能！](https://mp.weixin.qq.com/s/jzyXrNTEiT6jrvXYszkELQ)<br><br>

[6] SimROD: A Simple Adaptation Method for Robust Object Detection<br>
[paper](https://arxiv.org/abs/2107.13389)<br><br>

[5] Active Learning for Deep Object Detection via Probabilistic Modeling<br>
[paper](https://arxiv.org/abs/2103.16130)<br><br>

[4] Detecting Invisible People<br>
[paper](https://arxiv.org/abs/2012.08419) | [project](https://www.cs.cmu.edu/~tkhurana/invisible.htm) | [video](https://youtu.be/StEfnshXrCE)<br><br>

[3] Conditional Variational Capsule Network for Open Set Recognition<br>
[paper](https://arxiv.org/abs/2104.09159) | [code](https://github.com/guglielmocamporese/cvaecaposr)<br><br>

[2] MDETR : Modulated Detection for End-to-End Multi-Modal Understanding(Oral)<br>
[paper](https://arxiv.org/pdf/2104.12763) | [code](https://github.com/ashkamath/mdetr) | [project](https://ashkamath.github.io/mdetr_page/) | [colab](https://colab.research.google.com/github/ashkamath/mdetr/blob/colab/notebooks/MDETR_demo.ipynb)<br>
[解读：无需检测器提取特征！LeCun团队提出MDETR：实现真正的端到端多模态推理](https://zhuanlan.zhihu.com/p/394194465)<br><br>

[1] DetCo: Unsupervised Contrastive Learning for Object Detection<br>
[paper](https://arxiv.org/abs/2102.04803) | [code](https://github.com/xieenze/DetCo)<br>
[解读：性能优于何恺明团队MoCo v2，DetCo：为目标检测定制任务的对比学习](https://zhuanlan.zhihu.com/p/393163169)<br><br>

<a name="3DOD"/> 

### 3D目标检测(3D Object Detection)

[6] LIGA-Stereo: Learning LiDAR Geometry Aware Representations for Stereo-based 3D Detector<br>
[paper](https://arxiv.org/abs/2108.08258)<br><br>

[5] RandomRooms: Unsupervised Pre-training from Synthetic Shapes and Randomized Layouts for 3D Object Detection<br>
[paper](https://arxiv.org/abs/2108.07794)<br>

[4] Is Pseudo-Lidar needed for Monocular 3D Object detection?<br>
[paper](https://arxiv.org/abs/2108.06417)<br><br>

[3] Fog Simulation on Real LiDAR Point Clouds for 3D Object Detection in Adverse Weather<br>
[paper](https://arxiv.org/abs/2108.05249) | [code](http://www.trace.ethz.ch/lidar_fog_simulation)<br><br>

[2] Geometry Uncertainty Projection Network for Monocular 3D Object Detection<br>
[paper](https://arxiv.org/abs/2107.13774)<br><br>

[1] Unsupervised Domain Adaptive 3D Detection with Multi-Level Consistency<br>
[paper](https://arxiv.org/pdf/2107.11355.pdf)<br><br>

<a name="VOD"/>

### 视频目标检测(Video Object Detection)

[1] Social Fabric: Tubelet Compositions for Video Relation Detection<br>
[paper](https://arxiv.org/abs/2108.08363) | [code](https://github.com/shanshuo/Social-Fabric)<br><br>

<a name="HOI"/> 

### 人物交互检测(HOI Detection)

[1] Exploiting Scene Graphs for Human-Object Interaction Detection<br>
[paper](https://arxiv.org/abs/2108.08584) | [code](https://github.com/ht014/SG2HOI)<br><br>

<a name="SOD"/> 

### 显著性目标检测(Saliency Object Detection)

[2] Specificity-preserving RGB-D Saliency Detection<br>
[paper](https://arxiv.org/abs/2108.08162) | [code](https://github.com/taozh2017/SPNet)<br><br>

[1] Disentangled High Quality Salient Object Detection<br>
[paper](https://arxiv.org/abs/2108.03551)<br><br>

<a name="COD"/> 

### 伪装目标检测(Camouflaged Object Detection)

[1] TransForensics: Image Forgery Localization with Dense Self-Attention<br>
[paper](https://arxiv.org/abs/2108.03871)<br><br>

<a name="ADI"/> 

### 图像异常检测/表面缺陷检测(Anomally Detection in Image)

[2] DRÆM -- A discriminatively trained reconstruction embedding for surface anomaly detection<br>
[paper](https://arxiv.org/abs/2108.07610)<br><br>

[1] Divide-and-Assemble: Learning Block-wise Memory for Unsupervised Anomaly Detection<br>
[paper](https://arxiv.org/abs/2107.13118)<br><br>

<a name="EdgeDetection"/> 

### 边缘检测(Edge Detection)

[2] Pixel Difference Networks for Efficient Edge Detection<br>
[paper](https://arxiv.org/abs/2108.07009) | [code](https://github.com/zhuoinoulu/pidinet)<br><br>

[1] RINDNet: Edge Detection for Discontinuity in Reflectance, Illumination, Normal and Depth<br>
[paper](https://arxiv.org/abs/2108.00616)<br><br>


<br>
<a name="Segmentation"/> 

## 分割(Segmentation)

<a name="ImageSegmentation"/> 

### 图像分割(Image Segmentation)

[2] Labels4Free: Unsupervised Segmentation using StyleGAN<br>
[paper](https://arxiv.org/abs/2103.14968) | [code](https://rameenabdal.github.io/Labels4Free) | [project](https://rameenabdal.github.io/Labels4Free/)<br><br>

[1] Mining Latent Classes for Few-shot Segmentation(Oral)<br>
[paper](https://arxiv.org/abs/2103.15402) | [code](https://github.com/LiheYoung/MiningFSS)<br><br>

<a name="InstanceSegmentation"/> 

### 实例分割(Instance Segmentation)

[6] A Weakly Supervised Amodal Segmenter with Boundary Uncertainty Estimation<br>
[paper](https://arxiv.org/abs/2108.09897)<br><br>

[5] Instance Segmentation in 3D Scenes using Semantic Superpoint Tree Networks<br>
[paper](https://arxiv.org/abs/2108.07478) | [code](https://github.com/Gorilla-Lab-SCUT/SSTNet)<br><br>

[4] SOTR: Segmenting Objects with Transformers<br>
[paper](https://arxiv.org/abs/2108.06747) | [code](https://github.com/easton-cau/SOTR)<br><br>

[3] Hierarchical Aggregation for 3D Instance Segmentation<br>
[paper](https://arxiv.org/abs/2108.02350) | [code](https://github.com/hustvl/HAIS)<br><br>

[2] Crossover Learning for Fast Online Video Instance Segmentation<br>
[code](https://github.com/hustvl/CrossVIS)

[1] Instances as Queries<br>
[paper](https://arxiv.org/abs/2105.01928) | [code](https://github.com/hustvl/QueryInst)<br><br>

<a name="SemanticSegmentation"/> 

### 语义分割(Semantic Segmentation)

[21] Mining Contextual Information Beyond Image for Semantic Segmentation<br>
[paper](https://arxiv.org/abs/2108.11819)<br><br>

[20] Generalize then Adapt: Source-Free Domain Adaptive Semantic Segmentation<br>
[paper](https://arxiv.org/abs/2108.11249) | [project](http://sites.google.com/view/sfdaseg)<b><br>

[19] Self-Regulation for Semantic Segmentation<br>
[paper](https://arxiv.org/abs/2108.09702)<br><br>

[18] Multi-Anchor Active Domain Adaptation for Semantic Segmentation(Oral)<br>
[paper](https://arxiv.org/abs/2108.08012)<br><br>

[17] Multi-Target Adversarial Frameworks for Domain Adaptation in Semantic Segmentation<br>
[paper](https://arxiv.org/abs/2108.06962)<br><br>

[16] Exploiting a Joint Embedding Space for Generalized Zero-Shot Semantic Segmentation<br>
[paper](https://arxiv.org/abs/2108.06536)<br><br>

[15] LabOR: Labeling Only if Required for Domain Adaptive Semantic Segmentation(Oral)<br>
[paper](https://arxiv.org/abs/2108.05570)<br><br>

[14] Dual Path Learning for Domain Adaptation of Semantic Segmentation<br>
[paper](https://arxiv.org/abs/2108.06337) | [code](https://github.com/royee182/DPL)<br><br>

[13] Deep Metric Learning for Open World Semantic Segmentation<br>
[paper](https://arxiv.org/abs/2108.04562)<br><br>

[12] Complementary Patch for Weakly Supervised Semantic Segmentation<br>
[paper](https://arxiv.org/abs/2108.03852)<br><br>

[11] RECALL: Replay-based Continual Learning in Semantic Segmentation<br>
[paper](https://arxiv.org/abs/2108.03673)<br><br>

[10] Simpler is Better: Few-shot Semantic Segmentation with Classifier Weight Transformer<br>
[paper](https://arxiv.org/abs/2108.03032) ｜ [code](https://github.com/zhiheLu/CWT-for-FSS)<br><br>

[9] Learning Meta-class Memory for Few-Shot Semantic Segmentation<br>
[paper](https://arxiv.org/abs/2108.02958)<br><br>

[8] Personalized Image Semantic Segmentation<br>
[paper](https://arxiv.org/abs/2107.13978)<br><br>

[7] VMNet: Voxel-Mesh Network for Geodesic-Aware 3D Semantic Segmentation<br>
[paper](https://arxiv.org/abs/2107.13824) | [code](https://github.com/hzykent/VMNet)<br><br>

[6] Leveraging Auxiliary Tasks with Affinity Learning for Weakly Supervised Semantic Segmentation<br>
[paper](https://arxiv.org/abs/2107.11787)<br><br>

[5] ReDAL: Region-based and Diversity-aware Active Learning for Point Cloud Semantic Segmentation(点云语义分割)<br>
[paper](https://arxiv.org/abs/2107.11769)<br><br>

[4] Domain Adaptive Video Segmentation via Temporal Consistency Regularization(video semantic segmentation)<br>
[paper](https://arxiv.org/abs/2107.11004) | [code](https://github.com/Dayan-Guan/DA-VSN)<br><br>

[3] Standardized Max Logits: A Simple yet Effective Approach for Identifying Unexpected Road Obstacles in Urban-Scene Segmentation(Oral)<br>
[paper](https://arxiv.org/abs/2107.11264v1)<br><br>

[2] Re-distributing Biased Pseudo Labels for Semi-supervised Semantic Segmentation: A Baseline Investigation(Oral)<br>
[paper](https://arxiv.org/abs/2107.11279) | [code](https://github.com/CVMI-Lab/DARS)<br><br>

[1] Calibrated Adversarial Refinement for Stochastic Semantic Segmentation<br>
[paper](https://arxiv.org/abs/2006.13144) | [code](https://github.com/EliasKassapis/CARSSS)<br><br>


<a name="VOS"/> 

### 视频目标分割(Video Object Segmentation)

[2] Joint Inductive and Transductive Learning for Video Object Segmentation<br>
[paper](https://arxiv.org/abs/2108.03679) | [code](https://github.com/maoyunyao/JOINT)<br><br>

[1] Full-Duplex Strategy for Video Object Segmentation<br>
[paper](https://arxiv.org/abs/2108.03151) | [project](http://dpfan.net/FSNet/)<br><br>

<a name="RIS"/> 

### 参考图像分割(Referring Image Segmentation)

[1] Vision-Language Transformer and Query Generation for Referring Segmentation<br>
[paper](https://arxiv.org/abs/2108.05565) | [code](https://github.com/henghuiding/Vision-Language-Transformer)<br><br>

<a name="DensePrediction"/> 

### 密集预测(Dense Prediction)

[1] FaPN: Feature-aligned Pyramid Network for Dense Image Prediction<br>
[paper](https://arxiv.org/abs/2108.07058) | [code](https://github.com/EMI-Group/FaPN)<br><br>

<br>

<a name="Face"/> 

## 人脸(Face)

[1] Learning Facial Representations from the Cycle-consistency of Face<br>
[paper](https://arxiv.org/abs/2108.03427)<br><br>

<a name="FacialRecognition"/> 

### 人脸识别/检测(Facial Recognition/Detection)

[4] TransFER: Learning Relation-aware Facial Expression Representations with Transformers<br>
[paper](https://arxiv.org/abs/2108.11116)<br><br>

[3] Understanding and Mitigating Annotation Bias in Facial Expression Recognition<br>
[paper](https://arxiv.org/abs/2108.08504)<br><br>

[2] SynFace: Face Recognition with Synthetic Data<br>
[paper](https://arxiv.org/abs/2108.07960)<br><br>

[1] PASS: Protected Attribute Suppression System for Mitigating Bias in Face Recognition<br>
[paper](https://arxiv.org/abs/2108.03764)<br><br>

<a name="FaceSynthesis"/> 

### 人脸生成/合成/重建/编辑(Face Generation/Face Synthesis/Face Reconstruction/Face Editing)

[5] FACIAL: Synthesizing Dynamic Talking Face with Implicit Attribute Learning<br>
[paper](https://arxiv.org/abs/2108.07938)<br><br>

[4] Disentangled Lifespan Face Synthesis<br>
[paper](https://arxiv.org/abs/2108.02874) | [code](https://senhe.github.io/projects/iccv_2021_lifespan_face)<br><br>

[3] MeshTalk: 3D Face Animation from Speech using Cross-Modality Disentanglement(音频驱动面部动画)<br>
[paper](https://arxiv.org/abs/2104.08223) | [video](https://research.fb.com/wp-content/uploads/2021/04/mesh_talk.mp4)<br><br>

[2] Focal Frequency Loss for Image Reconstruction and Synthesis<br>
[paper](https://arxiv.org/abs/2012.12821) | [code](https://github.com/EndlessSora/focal-frequency-loss)<br><br>

[1] HeadGAN: One-shot Neural Head Synthesis and Editing<br>
[paper](https://arxiv.org/abs/2012.08261)<br><br>

<a name="FaceAnti-Spoofing"/> 

### 人脸伪造/反欺骗(Face Forgery/Face Anti-Spoofing)

[1] Exploring Temporal Coherence for More General Video Face Forgery Detection<br>
[paper](https://arxiv.org/abs/2108.06747)<br><br>


<br>

<a name="3DVision"/> 

## 三维视觉(3D Vision)

[3] Differentiable Surface Rendering via Non-Differentiable Sampling<br>
[paper](https://arxiv.org/abs/2108.04886)<br><br>

[2] M3D-VTON: A Monocular-to-3D Virtual Try-On Network(3D试穿)<br>
[paper](https://arxiv.org/abs/2108.05126)<br><br>

[1] Score-Based Point Cloud Denoising<br>
[paper](https://arxiv.org/abs/2107.10981v1)<br><br>


<a name="3DPC"/> 

### 点云(Point Cloud)
 
[14] A Robust Loss for Point Cloud Registration<br>
[paper](https://arxiv.org/abs/2108.11682)<br><br> 
 
[13] CSG-Stump: A Learning Friendly CSG-Like Representation for Interpretable Shape Parsing<br>
[paper](https://arxiv.org/abs/2108.11305) | [project](https://kimren227.github.io/projects/CSGStump/)<br><br>

[12] Voxel-based Network for Shape Completion by Leveraging Edge Generation<br>
[paper](https://arxiv.org/abs/2108.09936) | [code](https://github.com/xiaogangw/VE-PCN)<br><br>

[11] PoinTr: Diverse Point Cloud Completion with Geometry-Aware Transformers(点云补全)(Oral)<br>
[paper](https://arxiv.org/abs/2108.08839) | [code](https://github.com/yuxumin/PoinTr)<br><br>

[10] ME-PCN: Point Completion Conditioned on Mask Emptiness(点云补全)<br>
[paper](https://arxiv.org/abs/2108.08187)<br><br>

[9] Adaptive Graph Convolution for Point Cloud Analysis<br>
[paper](https://arxiv.org/abs/2108.08035) | [code](https://github.com/hrzhou2/AdaptConv-master)<br><br>

[8] PICCOLO: Point Cloud-Centric Omnidirectional Localization<br>
[paper](https://arxiv.org/abs/2108.06545)<br><br>

[7] AdaFit: Rethinking Learning-based Normal Estimation on Point Clouds<br>
[paper](https://arxiv.org/abs/2108.05836) | [code](https://runsong123.github.io/AdaFit/)<br><br>

[6] SnowflakeNet: Point Cloud Completion by Snowflake Point Deconvolution with Skip-Transformer<br>
[paper](https://arxiv.org/abs/2108.04444) | [code](https://github.com/AllenXiangX/SnowflakeNet)<br><br>

[5] DRINet: A Dual-Representation Iterative Learning Network for Point Cloud Segmentation<br>
[paper](https://arxiv.org/abs/2108.04023)<br><br>

[4] Unsupervised Learning of Fine Structure Generation for 3D Point Clouds by 2D Projection Matching<br>
[paper](https://arxiv.org/abs/2108.03746) | [code](https://github.com/chenchao15/2D)<br><br>

[3] (Just) A Spoonful of Refinements Helps the Registration Error Go Down(Oral)<br>
[paper](https://arxiv.org/abs/2108.03257)<br><br>

[2] Learning with Noisy Labels for Robust Point Cloud Segmentation(点云分割)<br>
[paper](https://arxiv.org/abs/2107.14230) | [code](https://shuquanye.com/PNAL_website/)<br><br>

[1] HRegNet: A Hierarchical Network for Large-scale Outdoor LiDAR Point Cloud Registration<br>
[paper](https://arxiv.org/abs/2107.11992) | [project](https://ispc-group.github.io/hregnet)<br><br> 

<a name="3DReconstruction"/> 

### 三维重建(3D Reconstruction)

[12] Patch2CAD: Patchwise Embedding Learning for In-the-Wild Shape Retrieval from a Single Image<br>
[paper](https://arxiv.org/abs/2108.09368)<br><br>

[11] Gravity-Aware Monocular 3D Human-Object Reconstruction<br>
[paper](https://arxiv.org/abs/2108.08844) | [code](http://4dqv.mpi-inf.mpg.de/GraviCap/)<br><br>

[10] 3DIAS: 3D Shape Reconstruction with Implicit Algebraic Surfaces<br>
[paper](https://arxiv.org/abs/2108.08653) | [code](https://myavartanoo.github.io/3dias/)<br><br>

[9] VolumeFusion: Deep Depth Fusion for 3D Scene Reconstruction<br>
[paper](https://arxiv.org/abs/2108.08623)<br><br>

[8] Learning Anchored Unsigned Distance Functions with Gradient Direction Alignment for Single-view Garment Reconstruction<br>
[paper](https://arxiv.org/abs/2108.08478)<br><br>

[7] Vis2Mesh: Efficient Mesh Reconstruction from Unstructured Point Clouds of Large Scenes with Learned Virtual View Visibility<br>
[paper](https://arxiv.org/abs/2108.08378) | [code](https://github.com/GDAOSU/vis2mesh)<br>

[6] Deep Hybrid Self-Prior for Full 3D Mesh Generation<br>
[paper](https://arxiv.org/abs/2108.08017) | [project](https://yqdch.github.io/DHSP3D)<br><br>

[5] PR-RRN: Pairwise-Regularized Residual-Recursive Networks for Non-rigid Structure-from-Motion<br>
[paper](https://arxiv.org/abs/2108.07506)<br><br>

[4] Learning Canonical 3D Object Representation for Fine-Grained Recognition<br>
[paper](https://arxiv.org/abs/2108.04628)<br><br>

[3] ELLIPSDF: Joint Object Pose and Shape Optimization with a Bi-level Ellipsoid and Signed Distance Function Description<br>
[paper](https://arxiv.org/abs/2108.00355)<br><br>

[2] Discovering 3D Parts from Image Collections<br>
[paper](https://arxiv.org/abs/2107.13629) | [project](https://chhankyao.github.io/lpd/)<br><br>

[1] PlaneTR: Structure-Guided Transformers for 3D Plane Recovery<br>
[paper](https://arxiv.org/abs/2107.13108) | [code](https://git.io/PlaneTR)<br><br>

<br>

<a name="NNS"/> 

## 神经网络设计与优化(Neural Network Structure Design & Optimization)

[2] Unifying Nonlocal Blocks for Neural Networks<br>
[paper](https://arxiv.org/abs/2108.02451)<br><br>

[1] Energy-Based Open-World Uncertainty Modeling for Confidence Calibration(置信度校准)<br>
[paper](https://arxiv.org/abs/2107.12628)<br><br>

<a name="CNN"/>

### CNN

[3] MicroNet: Improving Image Recognition with Extremely Low FLOPs<br>
[paper](https://arxiv.org/abs/2108.05894) | [code1](https://github.com/liyunsheng13/micronet) | [code2](https://github.com/liyunsheng13/micronet)<br><br>

[2] Learning to Resize Images for Computer Vision Tasks<br>
[paper](https://arxiv.org/abs/2103.09950)<br><br>

[1] Bias Loss for Mobile Neural Networks<br>
[paper](https://arxiv.org/abs/2107.11170)<br>
[解读：超越MobileNet V3 | 详解SkipNet+Bias Loss=轻量化模型新的里程碑](https://mp.weixin.qq.com/s/BSCSGdQrQ1F1-5ofuH5lTA)<br><br>

<a name="Attention"/> 

### Attention

[6] Causal Attention for Unbiased Visual Recognition<br>
[paper](https://arxiv.org/abs/2108.08782) | [code](https://github.com/Wangt-CN/CaaM)<br><br>

[5] Counterfactual Attention Learning for Fine-Grained Visual Categorization and Re-identification(因果推理)(细粒度识别)<br>
[paper](https://arxiv.org/abs/2108.08728) | [code](https://github.com/raoyongming/CAL)<br><br>

[4] Residual Attention: A Simple but Effective Method for Multi-Label Recognition<br>
[paper](https://arxiv.org/abs/2108.02456)<br><br>

[3] Fast Convergence of DETR with Spatially Modulated Co-Attention<br>
[paper](https://arxiv.org/abs/2108.02404) | [code](https://github.com/gaopengcuhk/SMCA-DETR)<br><br>

[2] SCOUTER: Slot Attention-based Classifier for Explainable Image Recognition<br>
[paper](https://arxiv.org/abs/2009.06138) | [code](https://github.com/wbw520/scouter)<br><br>

[1] FcaNet: Frequency Channel Attention Networks<br>
[paper](https://arxiv.org/abs/2012.11879) | [code](https://github.com/cfzd/FcaNet)<br><br>

<a name="Transformer"/> 

### Transformer

[10] An Empirical Study of Training Self-Supervised Vision Transformers(Oral)<br>
[paper](https://arxiv.org/abs/2104.02057)<br>
[解读：解决训练不稳定性，何恺明团队新作来了！自监督学习+Transformer=MoCoV3](https://mp.weixin.qq.com/s/L4a6ZHTPkzQPS5VVm6Hm-A)<br><br>

[9] LeViT: a Vision Transformer in ConvNet’s Clothing for Faster Inference<br>
[paper](https://arxiv.org/abs/2104.01136) | [code](https://github.com/facebookresearch/LeViT)<br>
[解读：FaceBook提出LeViT，0.077ms的单图处理速度却拥有ResNet50的精度](https://mp.weixin.qq.com/s/LtVANU9wHdzK_mNUO47yLA)<br><br>

[8] Emerging Properties in Self-Supervised Vision Transformers<br>
[paper](https://arxiv.org/abs/2104.14294) | [code](https://github.com/facebookresearch/dino)<br>
[解读：当Transformer遇见自监督学习！Facebook重磅开源DINO](https://mp.weixin.qq.com/s/1aV4dzmn55w0y5zu5b_uvA)<br><br>

[7] Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet<br>
[paper](https://arxiv.org/abs/2101.11986) | [code](https://github.com/yitu-opensource/T2T-ViT)<br>
[解读：ResNet被全面超越了，是Transformer干的：依图科技开源“可大可小”T2T-ViT，轻量版优于MobileNet](https://mp.weixin.qq.com/s/MjsarhMgKv4dvHJysdWFcA)<br><br>

[6] Vision Transformer with Progressive Sampling<br>
[paper](https://arxiv.org/abs/2108.01684) | [code](https://github.com/yuexy/PS-ViT)<br><br>

[5] Rethinking and Improving Relative Position Encoding for Vision Transformer<br>
[paper](https://arxiv.org/abs/2107.14222) | [code](https://github.com/microsoft/Cream/tree/main/iRPE)<br>
[解读：Vision Transformer中的相对位置编码](https://mp.weixin.qq.com/s/xwqzVb696GbXDxCVN0-cGA)<br><br>

[4] AutoFormer: Searching Transformers for Visual Recognition<br>
[paper](https://arxiv.org/abs/2107.00651) | [code](https://github.com/microsoft/AutoML)<br><br>

[3] Rethinking Spatial Dimensions of Vision Transformers<br>
[paper](https://arxiv.org/abs/2103.16302) | [code](https://github.com/naver-ai/pit)<br><br>

[2] Generic Attention-model Explainability for Interpreting Bi-Modal and Encoder-Decoder Transformers(Oral)<br>
[paper](https://arxiv.org/pdf/2103.15679.pdf) | [code](https://github.com/hila-chefer/Transformer-MM-Explainability)<br><br>

[1] Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions(Oral)<br>
[paper](https://arxiv.org/abs/2102.12122) | [code](https://github.com/whai362/PVT)<br>
[解读：金字塔视觉Transformer(PVT)：用于密集预测的多功能backbone](https://mp.weixin.qq.com/s/Ruqgm4lMcXveJkJUVCHrLg)<br><br>

<a name="NAS"/> 

### 神经网络架构搜索(NAS)

[4] Pi-NAS: Improving Neural Architecture Search by Reducing Supernet Training Consistency Shift<br>
[paper](https://arxiv.org/abs/2108.09671) | [code](https://github.com/Ernie1/Pi-NAS)<br><br>

[3] BN-NAS: Neural Architecture Search with Batch Normalization<br>
[paper](https://arxiv.org/abs/2108.07375)<br><br>

[2] NASOA: Towards Faster Task-oriented Online Fine-tuning with a Zoo of Models<br>
[paper](https://arxiv.org/abs/2108.03434)<br><br>

[1] AutoFormer: Searching Transformers for Visual Recognition<br>
[paper](https://arxiv.org/abs/2107.00651) | [code](https://github.com/microsoft/AutoML)<br><br>

<a name="LossFunction"/> 

### 损失函数(Loss Function)

[3] Rank & Sort Loss for Object Detection and Instance Segmentation(Oral)<br>
[paper](https://arxiv.org/abs/2107.11669) | [code](https://github.com/kemaloksuz/RankSortLoss)<br>
[解读：拒绝调参，显著提点！检测分割任务的新损失函数RS Loss开源](https://mp.weixin.qq.com/s/OPw4uGc_VFtHt98SgC1yJg)<br><br>

[2] Focal Frequency Loss for Image Reconstruction and Synthesis<br>
[paper](https://arxiv.org/abs/2012.12821) | [code](https://github.com/EndlessSora/focal-frequency-loss)<br><br>

[1] Orthogonal Projection Loss<br>
[paper](https://arxiv.org/abs/2103.14021) | [code](https://github.com/kahnchana/opl)<br><br>

<a name="Visualization"/> 

### 可视化/可解释性(Visualization/Interpretability)

[1] Finding Representative Interpretations on Convolutional Neural Networks<br>
[paper](https://arxiv.org/abs/2108.06384)<br><br>

<br>

<a name="ModelTraining"/> 

## 模型训练/泛化(Model Training/Generalization)

[3] MultiTask-CenterNet (MCN): Efficient and Diverse Multitask Learning using an Anchor Free Approach(多任务学习)<br>
[paper](https://arxiv.org/abs/2108.05060)<br><br>

[2] Impact of Aliasing on Generalization in Deep Convolutional Networks<br>
[paper](https://arxiv.org/abs/2108.03489)<br><br>

[1] Learning Compatible Embeddings<br>
[paper](https://arxiv.org/abs/2108.01958) | [code](https://github.com/IrvingMeng/LCE)<br><br>

<a name="NoisyLabel"/> 

### 噪声标签(Noisy Label)

[2] NGC: A Unified Framework for Learning with Open-World Noisy Data<br>
[paper](https://arxiv.org/abs/2108.11035)<br><br>

[1] Learning with Noisy Labels via Sparse Regularization<br>
[paper](https://arxiv.org/abs/2108.00192) | [code](https://github.com/hitcszx/lnl_sr)<br><br>

<a name="Long-Tailed"/> 

### 长尾分布(Long-Tailed Distribution)

[2] Learning of Visual Relations: The Devil is in the Tails<br>
[paper](https://arxiv.org/abs/2108.09668)<br><br>

[1] ACE: Ally Complementary Experts for Solving Long-Tailed Recognition in One-Shot(Oral)<br>
[paper](https://arxiv.org/abs/2108.02385) | [code](https://github.com/jrcai/ACE)<br><br>

<a name="OOD"/> 

### 分布外样本检测(Out of Distribution Detection)
 
[4] Semantically Coherent Out-of-Distribution Detection<br>
[paper](https://arxiv.org/abs/2108.11941) | [project](https://jingkang50.github.io/projects/scood)<br><br> 
 
[3] NGC: A Unified Framework for Learning with Open-World Noisy Data<br>
[paper](https://arxiv.org/abs/2108.11035)<br><br>

[2] Trash to Treasure: Harvesting OOD Data with Cross-Modal Matching for Open-Set Semi-Supervised Learning<br>
[paper](https://arxiv.org/abs/2108.05617)<br><br>

[1] CODEs: Chamfer Out-of-Distribution Examples against Overconfidence Issue<br>
[paper](https://arxiv.org/abs/2108.06024)<br><br>

<br>

<a name="ModelCompression"/> 

## 模型压缩(Model Compression)

<a name="KnowledgeDistillation"/> 

### 知识蒸馏(Knowledge Distillation)
 
[5] Multi-Task Self-Training for Learning General Representations(多任务学习)<br>
[paper](https://arxiv.org/abs/2108.11353)<br><br>
 
[4] G-DetKD: Towards General Distillation Framework for Object Detectors via Contrastive and Semantic-guided Feature Imitation<br>
[paper](https://arxiv.org/abs/2108.07482)<br><br>

[3] Online Multi-Granularity Distillation for GAN Compression<br>
[paper](https://arxiv.org/abs/2108.06908) | [code](https://github.com/bytedance/OMGD)<br><br>

[2] Distilling Holistic Knowledge with Graph Neural Networks<br>
[paper](https://arxiv.org/abs/2108.05507) | [code](https://github.com/wyc-ruiker/HKD)<br><br>

[1] AGKD-BML: Defense Against Adversarial Attack by Attention Guided Knowledge Distillation and Bi-directional Metric Learning<br>
[paper](https://arxiv.org/abs/2108.06017) | [code](https://github.com/hongw579/AGKD-BML)<br><br>

<a name="Pruning"/> 

### 剪枝(Pruning)
[1] ResRep: Lossless CNN Pruning via Decoupling Remembering and Forgetting<br>
[paper](https://arxiv.org/abs/2007.03260) | [code](https://github.com/DingXiaoH/ResRep)<br><br>
<a name="Pruning"/> 

### 剪枝(Pruning)

<a name="Quantization"/> 

### 量化(Quantization)

[2] Distance-aware Quantization<br>
[paper](https://arxiv.org/abs/2108.06983)<br><br>

[1] Generalizable Mixed-Precision Quantization via Attribution Rank Preservation<br>
[paper](https://arxiv.org/abs/2108.02720) | [code](https://github.com/ZiweiWangTHU/GMPQ.git)<br><br>

<br>

<a name="IGIS"/> 


## 图像生成/合成(Image Generation/Image Synthesis)

[9] Image Inpainting via Conditional Texture and Structure Dual Generation<br>
[paper](https://arxiv.org/abs/2108.09760) | [code](https://github.com/Xiefan-Guo/CTSDG)<br><br>

[8] Dual Projection Generative Adversarial Networks for Conditional Image Generation<br>
[paper](https://arxiv.org/abs/2108.09016)<br><br>

[7] Speech Drives Templates: Co-Speech Gesture Synthesis with Learned Templates(手势生成)<br>
[paper](https://arxiv.org/abs/2108.08020) | [code](https://github.com/ShenhanQian/SpeechDrivesTemplates)<br><br>

[6] Orthogonal Jacobian Regularization for Unsupervised Disentanglement in Image Generation<br>
[paper](https://arxiv.org/abs/2108.07668) | [code](https://github.com/csyxwei/OroJaR)<br><br>

[5] ILVR: Conditioning Method for Denoising Diffusion Probabilistic Models(Oral)<br>
[paper](https://arxiv.org/abs/2108.02938)<br><br>

[4] Toward Spatially Unbiased Generative Models<br>
[paper](https://arxiv.org/abs/2108.01285)<br><br>

[3] A Light Stage on Every Desk<br>
[paper](https://arxiv.org/abs/2105.08051) | [project](https://grail.cs.washington.edu/projects/Light_Stage_on_Every_Desk/)<br><br>

[2] Handwriting Transformers<br>
[paper](https://arxiv.org/abs/2104.03964)<br><br>

[1] On Generating Transferable Targeted Perturbations<br>
[paper](https://arxiv.org/abs/2103.14641) | [code](https://github.com/Muzammal-Naseer/TTP)<br><br>

<a name="ViewSynthesis"/> 

### 视图合成(View Synthesis)

[1] PixelSynth: Generating a 3D-Consistent Experience from a Single Image<br>
[paper](https://arxiv.org/abs/2108.05892) | [project](https://crockwell.github.io/pixelsynth/)<br><br>

<br>

<a name="GAN"/> 

## GAN/生成式/对抗式(GAN/Generative/Adversarial)

[15] Towards Vivid and Diverse Image Colorization with Generative Color Prior(图像着色)<br>
[paper](https://arxiv.org/abs/2108.08826)<br><br>

[14] Exploiting Multi-Object Relationships for Detecting Adversarial Attacks in Complex Scenes<br>
[paper](https://arxiv.org/abs/2108.08421)<br><br>

[13] Unsupervised Geodesic-preserved Generative Adversarial Networks for Unconstrained 3D Pose Transfer<br>
[paper](https://arxiv.org/abs/2108.07520) ｜ [code](https://github.com/mikecheninoulu/Unsupervised_IEPGAN)<br><br>

[12] Online Multi-Granularity Distillation for GAN Compression<br>
[paper](https://arxiv.org/abs/2108.06908) | [code](https://github.com/bytedance/OMGD)<br><br>

[11] AGKD-BML: Defense Against Adversarial Attack by Attention Guided Knowledge Distillation and Bi-directional Metric Learning<br>
[paper](https://arxiv.org/abs/2108.06017) | [code](https://github.com/hongw579/AGKD-BML)<br><br> 

[10] Meta Gradient Adversarial Attack<br>
[paper](https://arxiv.org/abs/2108.04204)<br><br>

[9] Sketch Your Own GAN<br>
[paper](https://arxiv.org/abs/2108.02774) | [code](https://peterwang512.github.io/) | [project](https://peterwang512.github.io/GANSketching)<br>
[解读：用一张草图创建GAN模型，新手也能玩转，朱俊彦团队新研究入选ICCV 2021](https://mp.weixin.qq.com/s/pXsMJEhHRdYFYOqd4oYiCQ)<br><br>

[8] Feature Importance-aware Transferable Adversarial Attacks<br>
[paper](https://arxiv.org/abs/2107.14185) | [code](https://github.com/hcguoO0/FIA)<br><br>

[7] From Continuity to Editability: Inverting GANs with Consecutive Images<br>
[paper](https://arxiv.org/abs/2107.13812) | [code](https://github.com/cnnlstm/InvertingGANs_with_ConsecutiveImgs)<br><br>

[6] Learnable Boundary Guided Adversarial Training<br>
[paper](https://arxiv.org/abs/2011.11164) | [code](https://github.com/jiequancui/LBGAT)<br><br>

[5] Transporting Causal Mechanisms for Unsupervised Domain Adaptation(Oral)<br>
[paper](https://arxiv.org/abs/2107.11055)<br><br>

[4] Robustness via Cross-Domain Ensembles(Oral)<br>
[paper](https://crossdomain-ensembles.epfl.ch/#paper) | [code](https://github.com/EPFL-VILAB/XDEnsembles) | [model](https://github.com/EPFL-VILAB/XDEnsembles#pretrained-models) | [homepage](https://crossdomain-ensembles.epfl.ch/) | [video](https://youtu.be/h0FI5Sp7y7g)<br><br>

[3] HeadGAN: One-shot Neural Head Synthesis and Editing<br>
[paper](https://arxiv.org/abs/2012.08261)<br><br>

[2] Labels4Free: Unsupervised Segmentation using StyleGAN<br>
[paper](https://arxiv.org/abs/2103.14968) | [code](https://rameenabdal.github.io/Labels4Free) | [project](https://rameenabdal.github.io/Labels4Free/)<br><br>

[1] EigenGAN: Layer-Wise Eigen-Learning for GANs<br>
[paper](https://arxiv.org/abs/2104.12476) | [code](https://github.com/LynnHo/EigenGAN-Tensorflow)



<br>

<a name="ImageProcessing"/> 

## 图像处理(Image Processing)

[4] Towards Vivid and Diverse Image Colorization with Generative Color Prior(图像着色)<br>
[paper](https://arxiv.org/abs/2108.08826)<br><br>

[3] Hierarchical Conditional Flow: A Unified Framework for Image Super-Resolution and Image Rescaling<br>
[paper](https://arxiv.org/abs/2108.05301) | [code](https://github.com/JingyunLiang/HCFlow)<br><br>

[2] Accelerating Atmospheric Turbulence Simulation via Learned Phase-to-Space Transform<br>
[paper](https://arxiv.org/abs/2107.11627)<br><br>

[1] Equivariant Imaging: Learning Beyond the Range Space(Oral)<br>
[paper](https://arxiv.org/pdf/2103.14756.pdf)<br><br>

<a name="SuperResolution"/> 

### 超分辨率(Super Resolution)

[2] Mutual Affine Network for Spatially Variant Kernel Estimation in Blind Image Super-Resolution<br>
[paper](https://arxiv.org/abs/2108.05302) | [code](https://github.com/JingyunLiang/MANet)<br><br>

[1] Learning for Scale-Arbitrary Super-Resolution from Scale-Specific Networks<br>
[paper](https://arxiv.org/abs/2004.03791) | [code](https://github.com/LongguangWang/ArbSR)<br><br>

<a name="ImageRestoration"/> 

### 图像复原/图像增强(Image Restoration)

[2] Real-time Image Enhancer via Learnable Spatial-aware 3D Lookup Tables<br>
[paper](https://arxiv.org/abs/2108.08697)<br><br>

[1] Spatially-Adaptive Image Restoration using Distortion-Guided Networks<br>
[paper](https://arxiv.org/abs/2108.08617) | [code](https://github.com/human-analysis/spatially-adaptive-image-restoration)<br><br>

<a name="ISR"/> 

### 图像去阴影/去反射(Image Shadow Removal/Image Reflection Removal)

[1] CANet: A Context-Aware Network for Shadow Removal<br>
[paper](https://arxiv.org/abs/2108.09894)<br><br>

<a name="ImageDenoising"/> 

### 图像去噪/去模糊/去雨去雾(Image Denoising)

[1] Rethinking Coarse-to-Fine Approach in Single Image Deblurring<br>
[paper](https://arxiv.org/abs/2108.05054) | [code](https://github.com/chosj95/MIMO-UNet)<br><br>

<a name="ImageEdit"/> 

### 图像编辑/修复(Image Edit/Image Inpainting)

[2] GAN Inversion for Out-of-Range Images with Geometric Transformations<br>
[paper](https://arxiv.org/abs/2108.08998) | [code](https://kkang831.github.io/publication/ICCV_2021_BDInvert/)<br><br>

[1] Occlusion-Aware Video Object Inpainting(视频修复)<br>
[paper](https://arxiv.org/abs/2108.06765)<br><br>

<a name="StyleTransfer"/> 

### 风格迁移(Style Transfer)

[5] SSH: A Self-Supervised Framework for Image Harmonization(图像协调)<br>
[paper](https://arxiv.org/abs/2108.06805) | [code](https://github.com/VITA-Group/SSHarmonization)<br><br>

[4] Domain-Aware Universal Style Transfer<br>
[paper](https://arxiv.org/abs/2108.04441)<br><br>

[3] AdaAttN: Revisit Attention Mechanism in Arbitrary Neural Style Transfer<br>
[paper](https://arxiv.org/abs/2108.03647) | [code1](https://github.com/PaddlePaddle/PaddleGAN) | [code2](https://github.com/Huage001/AdaAttN)<br><br>

[2] ALADIN: All Layer Adaptive Instance Normalization for Fine-grained Style Similarity(风格迁移)<br>
[paper](https://arxiv.org/abs/2103.09776)<br><br>

[1] Multiple Heads are Better than One: Few-shot Font Generation with Multiple Localized Experts(字体生成)<br>
[paper](https://arxiv.org/abs/2104.00887) | [code](https://github.com/clovaai/mxfont)<br><br>

<a name="IQA"/> 

### 图像质量评估(Image Quality Assessment)

[1] MUSIQ: Multi-scale Image Quality Transformer<br>
[paper](https://arxiv.org/abs/2108.05997)<br><br>

<a name="NeuralVideoDelivery"/> 
 
### 网络视频传输(Neural Video Delivery)
 
[1] Overfitting the Data: Compact Neural Video Delivery via Content-aware Feature Modulation<br>
[paper](https://arxiv.org/abs/2108.08202) | [code](https://github.com/Neural-video-delivery/CaFM-Pytorch-ICCV2021)<br><br>
 
<br>

<a name="Estimation"/> 

## 估计(Estimation)

<a name="HumanPoseEstimation"/> 

### 姿态估计(Human Pose Estimation)
 
[10] Probabilistic Modeling for Human Mesh Recovery<br>
[paper](https://arxiv.org/abs/2108.11944) | [code](https://www.seas.upenn.edu/~nkolot/projects/prohmr)<br><br> 

[9] DECA: Deep viewpoint-Equivariant human pose estimation using Capsule Autoencoders(Oral)<br>
[paper](https://arxiv.org/abs/2108.08557) | [code](https://github.com/mmlab-cv/DECA)<br><br>

[8] Learning Skeletal Graph Neural Networks for Hard 3D Pose Estimation<br>
[paper](https://arxiv.org/abs/2108.07181) | [code](https://github.com/ailingzengzzz/Skeletal-GNN)<br><br>

[7] EventHPE: Event-based 3D Human Pose and Shape Estimation<br>
[paper](https://arxiv.org/abs/2108.06819)<br><br>

[6] HandFoldingNet: A 3D Hand Pose Estimation Network Using Multiscale-Feature Guided Folding of a 2D Hand Skeleton<br>
[paper](https://arxiv.org/abs/2108.05545) | [code](https://github.com/cwc1260/HandFold)<br><br>

[5] Online Knowledge Distillation for Efficient Pose Estimation<br>
[paper](https://arxiv.org/abs/2108.02092)<br><br>

[4] Probabilistic Monocular 3D Human Pose Estimation with Normalizing Flows<br>
[paper](https://arxiv.org/abs/2107.13788)<br><br>

[3] Human Pose Regression with Residual Log-likelihood Estimation(Oral)<br>
[paper](https://arxiv.org/abs/2107.11291) | [code](https://github.com/Jeff-sjtu/res-loglikelihood-regression)<br><br>

[2] PyMAF: 3D Human Pose and Shape Regression with Pyramidal Mesh Alignment Feedback Loop(Oral)<br>
[paper](https://arxiv.org/pdf/2103.16507.pdf) | [code](https://github.com/HongwenZhang/PyMAF) | [project](https://hongwenzhang.github.io/pymaf)<br><br>

[1] HuMoR: 3D Human Motion Model for Robust Pose Estimation(Oral)<br>
[paper](https://geometry.stanford.edu/projects/humor/docs/humor.pdf) | [video](https://youtu.be/5VWirxUHG0Y) | [project](https://geometry.stanford.edu/projects/humor/)<br><br>

<a name="Flow/Pose/MotionEstimation"/> 

### 光流/位姿/运动估计(Flow/Pose/Motion Estimation)

[1] SO-Pose: Exploiting Self-Occlusion for Direct 6D Pose Estimation<br>
[paper](https://arxiv.org/abs/2108.08367)<br><br>

<a name="DepthEstimation"/> 

### 深度估计(Depth Estimation)

[7] PatchMatch-RL: Deep MVS with Pixelwise Depth, Normal, and Visibility(Oral)<br>
[paper](https://arxiv.org/abs/2108.08943) | [code](https://github.com/leejaeyong7/patchmatch-rl)<br><br>

[6] Fine-grained Semantics-aware Representation Enhancement for Self-supervised Monocular Depth Estimation(Oral)<br>
[paper](https://arxiv.org/abs/2108.08829) | [code](https://github.com/hyBlue/FSRE-Depth)<br><br>

[5] StructDepth: Leveraging the structural regularities for self-supervised indoor depth estimation<br>
[paper](https://arxiv.org/abs/2108.08574) | [code](https://github.com/SJTU-ViSYS/StructDepth)<br><br>

[4] Self-supervised Monocular Depth Estimation for All Day Images using Domain Separation<br>
[paper](https://arxiv.org/abs/2108.07628)<br><br>

[3] Towards Interpretable Deep Networks for Monocular Depth Estimation<br>
[paper](https://arxiv.org/abs/2108.05312) | [code](https://github.com/youzunzhi/InterpretableMDE)<br><br>

[2] Regularizing Nighttime Weirdness: Efficient Self-supervised Monocular Depth Estimation in the Dark<br>
[paper](https://arxiv.org/abs/2108.03830)<br><br>

[1] MonoIndoor: Towards Good Practice of Self-Supervised Monocular Depth Estimation for Indoor Environments<br>
[paper](https://arxiv.org/abs/2107.12429)<br><br>

<br>
<a name="ImageRetrieval"/> 

## 图像&视频检索/理解(Image&Video Retrieval/Video Understanding)
 
[7] Cross-category Video Highlight Detection via Set-based Learning(视频高光检测)<br>
[paper](https://arxiv.org/abs/2108.11770) | [code](https://github.com/ChrisAllenMing/Cross_Category_Video_Highlight)<br><br> 

[6] Universal Cross-Domain Retrieval: Generalizing Across Classes and Domains<br>
[paper](https://arxiv.org/abs/2108.08356)<br><br>

[5] ASMR: Learning Attribute-Based Person Search with Adaptive Semantic Margin Regularizer<br>
[paper](https://arxiv.org/abs/2108.04533)<br><br>

[4] Image Retrieval on Real-life Images with Pre-trained Vision-and-Language Models<br>
[paper](https://arxiv.org/abs/2108.04024) | [code](https://cuberick-orion.github.io/CIRR/)<br><br>

[3] DOLG: Single-Stage Image Retrieval with Deep Orthogonal Fusion of Local and Global Features<br>
[paper](https://arxiv.org/abs/2108.02927)<br><br>

[2] Hand Image Understanding via Deep Multi-Task Learning(手部图像理解)<br>
[paper](https://arxiv.org/abs/2107.11646)<br><br>

[1] Cross-Sentence Temporal and Semantic Relations in Video Activity Localisation<br>
[paper](https://arxiv.org/abs/2107.11443)<br><br>

<a name="ActionRecognition"/> 

### 行为识别/行为识别/动作识别/检测/分割(Action/Activity Recognition)
 
[8] Spatio-Temporal Dynamic Inference Network for Group Activity Recognition<br>
[paper](https://arxiv.org/abs/2108.11743) | [code](https://github.com/JacobYuan7/DIN_GAR)<br><br> 

[7] Group-aware Contrastive Regression for Action Quality Assessment(动作质量评估)<br>
[paper](https://arxiv.org/abs/2108.07797)<br><br>

[6] Foreground-Action Consistency Network for Weakly Supervised Temporal Action Localization(动作定位)<br>
[paper](https://arxiv.org/abs/2108.06524) | [code](https://github.com/LeonHLJ/FAC-Net)<br><br>

[5] Learning Action Completeness from Points for Weakly-supervised Temporal Action Localization(动作定位)<br>
[paper](https://arxiv.org/abs/2108.05029) | [code](https://github.com/Pilhyeon)<br><br>

[4] Elaborative Rehearsal for Zero-shot Action Recognition<br>
[paper](https://arxiv.org/abs/2108.02833) | [code](https://github.com/DeLightCMU/ElaborativeRehearsal)<br><br>

[3] Skeleton Cloud Colorization for Unsupervised 3D Action Representation Learning<br>
[paper](https://arxiv.org/abs/2108.01959)<br><br>

[2] Enriching Local and Global Contexts for Temporal Action Localization<br>
[paper](https://arxiv.org/abs/2107.12960)<br><br>

[1] Channel-wise Topology Refinement Graph Convolution for Skeleton-Based Action Recognition<br>
[paper](https://arxiv.org/abs/2107.12213) | [code](https://github.com/Uason-Chen/CTR-GCN)<br><br>

<a name="Re-Identification"/> 

### 行人重识别/检测(Re-Identification/Detection)

[7] Multi-Expert Adversarial Attack Detection in Person Re-identification Using Context Inconsistency<br>
[paper](https://arxiv.org/abs/2108.09891)<br><br>

[6] Learning by Aligning: Visible-Infrared Person Re-identification using Cross-Modal Correspondences<br>
[paper](https://arxiv.org/abs/2108.07422)<br><br>

[5] Towards Discriminative Representation Learning for Unsupervised Person Re-identification<br>
[paper](https://arxiv.org/abs/2108.03439)<br><br>

[4] Learning Instance-level Spatial-Temporal Patterns for Person Re-identification<br>
[paper](https://arxiv.org/abs/2108.00171) | [Cleaned database](https://github.com/RenMin1991/cleaned-DukeMTMC-reID/)<br><br>

[3] An Intermediate Domain Module for Domain Adaptive Person Re-ID(Oral)<br>
[paper](https://arxiv.org/abs/2108.02413) | [code](https://github.com/SikaStar/IDM)<br><br>

[2] Spatio-Temporal Representation Factorization for Video-based Person Re-Identification<br>
[paper](https://arxiv.org/abs/2107.11878)<br><br>

[1] TransReID: Transformer-based Object Re-Identification<br>
[paper](https://arxiv.org/abs/2102.04378) | [code](https://github.com/heshuting555/TransReID)<br>
[解读：来自Transformer的降维打击：ReID各项任务全面领先，阿里&浙大提出TransReID](https://mp.weixin.qq.com/s/rATLyYBgo2nWY4rKXmgV5w)<br><br>

<a name="VideoCaption"/> 

### 图像/视频字幕(Image/Video Caption)

[1] End-to-End Dense Video Captioning with Parallel Decoding<br>
[paper](https://arxiv.org/abs/2108.07781) | [code](https://github.com/ttengwang/PDVC)<br><br>

<br>

<a name="VisualLocalization"/> 

## 视觉定位(Visual Localization)
 
[5] Few-shot Visual Relationship Co-localization<br>
[paper](https://arxiv.org/abs/2108.11618)<br><br>
 
[4] PICCOLO: Point Cloud-Centric Omnidirectional Localization<br>
[paper](https://arxiv.org/abs/2108.06545)<br><br>

[3] Normalization Matters in Weakly Supervised Object Localization<br>
[paper](https://arxiv.org/abs/2107.13221)<br><br>

[2] TS-CAM: Token Semantic Coupled Attention Map for Weakly Supervised Object Localization<br>
[paper](https://arxiv.org/abs/2103.14862) | [code](https://github.com/vasgaowei/TS-CAM)<br><br>

[1] Boundary-sensitive Pre-training for Temporal Localization in Videos<br>
[paper](https://arxiv.org/abs/2011.10830)<br><br>

<a name="ImageMatching"/> 

### 图像匹配(Image Matching)

[6] Learning to Match Features with Seeded Graph Matching Network<br>
[paper](https://arxiv.org/abs/2108.08771) | [code](https://github.com/vdvchen/SGMNet)<br><br>

[5] Pixel-Perfect Structure-from-Motion with Featuremetric Refinement<br>
[paper](https://arxiv.org/abs/2108.08291) | [code](https://github.com/cvg/pixel-perfect-sfm)<br><br>

[4] Progressive Correspondence Pruning by Consensus Learning<br>
[paper](https://arxiv.org/abs/2101.00591) | [code](https://github.com/sailor-z/CLNet) | [project](https://sailor-z.github.io/projects/CLNet.html)<br>
[解读：CLNet：基于一致性学习的渐进式匹配筛选](https://zhuanlan.zhihu.com/p/394483122)<br><br>

[3] Multi-scale Matching Networks for Semantic Correspondence<br>
[paper](https://arxiv.org/abs/2108.00211)<br><br>

[2] Warp Consistency for Unsupervised Learning of Dense Correspondences(Oral)<br>
[paper](https://arxiv.org/abs/2104.03308) | [code](https://github.com/PruneTruong/DenseMatching)<br><br>

[1] COTR: Correspondence Transformer for Matching Across Images<br>
[paper](https://arxiv.org/abs/2103.14167)<br><br>

<br>

<a name="3DVision"/> 

## 三维视觉(3D Vision)
 
[2] Unsupervised Dense Deformation Embedding Network for Template-Free Shape Correspondence<br>
[paper](https://arxiv.org/abs/2108.11609)<br><br>

[1] MVTN: Multi-View Transformation Network for 3D Shape Recognition<br>
[paper](https://arxiv.org/abs/2011.13244)<br><br>

<br>

<a name="ObjectTracking"/> 

## 目标跟踪(Object Tracking)

[9] MOTSynth: How Can Synthetic Data Help Pedestrian Detection and Tracking?<br>
[paper](https://arxiv.org/abs/2108.09518)<br><br>

[8] Learning Spatio-Temporal Transformer for Visual Tracking<br>
[paper](https://arxiv.org/abs/2103.17154) | [code](https://github.com/researchmm/Stark)<br>
[解读：屠榜目标跟踪！大连理工和MSRA提出STARK：基于Transformer的目标跟踪器](https://mp.weixin.qq.com/s/Tlrhoj2jnexu9mjRky0yww)<br><br>

[7] Box-Aware Feature Enhancement for Single Object Tracking on Point Clouds<br>
[paper](https://arxiv.org/abs/2108.04728)<br><br>

[6] Video Annotation for Visual Tracking via Selection and Refinement<br>
[paper](https://arxiv.org/abs/2108.03821)<br><br>

[5] Saliency-Associated Object Tracking<br>
[paper](https://arxiv.org/abs/2108.03637)<br><br>

[4] Learn to Match: Automatic Matching Network Design for Visual Tracking<br>
[paper](https://arxiv.org/abs/2108.00803) | [code](https://github.com/JudasDie/SOTS)<br><br>

[3] HiFT: Hierarchical Feature Transformer for Aerial Tracking<br>
[paper](https://arxiv.org/abs/2108.00202) | [code](https://github.com/vision4robotics/HiFT)<br><br>

[2] Learning to Adversarially Blur Visual Object Tracking<br>
[paper](https://arxiv.org/abs/2107.12085) | [code](https://github.com/tsingqguo/ABA)<br><br>

[1] Detecting Invisible People<br>
[paper](https://arxiv.org/abs/2012.08419) | [project](https://www.cs.cmu.edu/~tkhurana/invisible.htm) | [video](https://youtu.be/StEfnshXrCE)<br><br>

<br>

<a name="MedicalImaging"/> 

## 医学影像(Medical Imaging)

[2] Recurrent Mask Refinement for Few-Shot Medical Image Segmentation<br>
[paper](https://arxiv.org/abs/2108.00622)<br><br>

[1] Generative Adversarial Registration for Improved Conditional Deformable Templates<br>
[paper](https://arxiv.org/abs/2105.04349) | [code](https://github.com/neel-dey/Atlas-GAN) | [homepage](https://www.neeldey.com/deformable-templates/)<br><br>

<br>

<a name="TDR"/> 

## 文本检测/识别(Text Detection/Recognition)

[5] From Two to One: A New Scene Text Recognizer with Visual Language Modeling Network<br>
[paper](https://arxiv.org/abs/2108.09661) | [code&dataset](https://github.com/wangyuxin87/VisionLAN)<br><br>

[4] Adaptive Boundary Proposal Network for Arbitrary Shape Text Detection<br>
[paper](https://arxiv.org/abs/2107.12664)<br><br>

[3] Joint Visual Semantic Reasoning: Multi-Stage Decoder for Text Recognition<br>
[paper](https://arxiv.org/abs/2107.12090)<br><br>

[2] Text is Text, No Matter What: Unifying Text Recognition using Knowledge Distillation<br>
[paper](https://arxiv.org/abs/2107.12087)<br><br>

[1] Towards the Unseen: Iterative Text Recognition by Distilling from Errors<br>
[paper](https://arxiv.org/abs/2107.12081)<br><br>

<br>

<a name="RSI"/> 

## 遥感图像(Remote Sensing Image)

[4] Structured Outdoor Architecture Reconstruction by Exploration and Classification<br>
[paper](https://arxiv.org/abs/2108.07990)<br><br>

[3] Change is Everywhere Single-Temporal Supervised Object Change Detection for High Spatial Resolution Remote Sensing Imagery(变化检测)<br>
[paper](https://arxiv.org/abs/2108.07002) | [code](https://github.com/Z-Zheng/ChangeStar)<br><br>

[2] Geography-Aware Self-Supervised Learning<br>
[paper](https://arxiv.org/abs/2011.09980)<br><br>

[1] Seasonal Contrast: Unsupervised Pre-Training from Uncurated Remote Sensing Data(迁移学习)<br>
[paper](https://arxiv.org/abs/2103.16607) ｜ [code](https://github.com/ElementAI/seasonal-contrast)<br><br>

<br>

<a name="SG"/> 

## 场景图(Scene Graph)

<a name="SGG"/> 

### 场景图生成(Scene Graph Generation)

[5] Graph-to-3D: End-to-End Generation and Manipulation of 3D Scenes Using Scene Graphs<br>
[paper](https://arxiv.org/abs/2108.08841)<br><br>

[4] Target Adaptive Context Aggregation for Video Scene Graph Generation<br>
[paper](https://arxiv.org/abs/2108.08121) | [code](https://github.com/MCG-NJU/TRACE)<br><br>

[3] Unconditional Scene Graph Generation<br>
[paper](https://arxiv.org/abs/2108.05884)<br><br>

[2] Spatial-Temporal Transformer for Dynamic Scene Graph Generation<br>
[paper](https://arxiv.org/abs/2107.12309)<br>
[解读：用于视频场景图生成的时空上下文Transformer](https://zhuanlan.zhihu.com/p/393637591)<br><br>

[1] Unconstrained Scene Generation with Locally Conditioned Radiance Fields<br>
[paper](https://arxiv.org/abs/2104.00670)<br><br>

<a name="SGP"/> 

### 场景图预测(Scene Graph Prediction)

[1] Generative Compositional Augmentations for Scene Graph Prediction<br>
[paper](https://arxiv.org/abs/2007.05756) | [code](https://github.com/bknyaz/sgg)<br><br>

<br>

<a name="DataProcessing"/> 

## 数据处理(Data Processing)

<a name="DataAugmentation"/> 

### 数据增广(Data Augmentation)

[3] BiaSwap: Removing dataset bias with bias-tailored swapping augmentation<br>
[paper](https://arxiv.org/abs/2108.10008)<br><br>

[2] Amplitude-Phase Recombination: Rethinking Robustness of Convolutional Neural Networks in Frequency Domain<br>
[paper](https://arxiv.org/abs/2108.08487) | [code](https://github/iCGY96/APR)<br><br>

[1] MixMo: Mixing Multiple Inputs for Multiple Outputs via Deep Subnetworks<br>
[paper](https://arxiv.org/abs/2103.06132)<br>
[解读：“白嫖”性能的MixMo，一种新的数据增强or模型融合方法](https://zhuanlan.zhihu.com/p/396554361)<br><br>

<a name="AnomalyDetection"/> 

### 异常检测(Anomaly Detection)

[3] A Hybrid Video Anomaly Detection Framework via Memory-Augmented Flow Reconstruction and Flow-Guided Frame Prediction<br>
[paper](https://arxiv.org/abs/2108.06852) | [code](https://github.com/LiUzHiAn/hf2vad)<br><br>

[2] Weakly Supervised Temporal Anomaly Segmentation with Dynamic Time Warping<br>
[paper](https://arxiv.org/abs/2108.06816)<br><br>

[1] Weakly-supervised Video Anomaly Detection with Robust Temporal Feature Magnitude Learning<br>
[paper](https://arxiv.org/abs/2101.10030) | [code](https://github.com/tianyu0207/RTFM)<br><br>

<a name="RepresentationLearning"/> 

### 表征学习(Representation Learning)

[4] Self-Supervised Visual Representations Learning by Contrastive Mask Prediction<br>
[paper](https://arxiv.org/abs/2108.07954)<br><br>

[3] Collaborative Unsupervised Visual Representation Learning from Decentralized Data<br>
[paper](https://arxiv.org/abs/2108.06492)<br><br>

[2] Enhancing Self-supervised Video Representation Learning via Multi-level Feature Optimization<br>
[paper](https://arxiv.org/abs/2108.02183)<br><br>

[1] In-Place Scene Labelling and Understanding with Implicit Scene Representation(Oral)<br>
[paper](https://arxiv.org/abs/2103.15875) | [project](https://shuaifengzhi.com/Semantic-NeRF/)<br><br>

<a name="ImageCompression"/> 

### 图像压缩(Image Compression)

[1] Variable-Rate Deep Image Compression through Spatially-Adaptive Feature Transform<br>
[paper](https://arxiv.org/abs/2108.09551) | [code](https://github.com/micmic123/QmapCompression)<br><br>

<a name="BatchNormalization"/> 

### 归一化/正则化(Batch Normalization)

<a name="ImageClustering"/> 

### 图像聚类(Image Clustering)

[5] A Unified Objective for Novel Class Discovery(Oral)<br>
[paper](https://arxiv.org/abs/2108.08536) | [code](https://ncd-uno.github.io/)<br><br>

[4] Instance Similarity Learning for Unsupervised Feature Representation<br>
[paper](https://arxiv.org/abs/2108.02721) | [code](https://github.com/ZiweiWangTHU/ISL.git)<br><br>

[3] Graph Constrained Data Representation Learning for Human Motion Segmentation(人体运动分割)<br>
[paper](https://arxiv.org/abs/2107.13362)<br><br>

[2] Improve Unsupervised Pretraining for Few-label Transfer<br>
[paper](https://arxiv.org/abs/2107.12369)<br><br>

[1] Clustering by Maximizing Mutual Information Across Views<br>
[paper](https://arxiv.org/abs/2107.11635)<br><br>

<br>

<a name="Few-shotLearning"/> 

## 小样本学习/零样本学习(Few-shot/Zero-shot Learning)
 
[6] Binocular Mutual Learning for Improving Few-shot Classification<br>
[paper](https://arxiv.org/abs/2108.12104)<br><br>

[5] Field-Guide-Inspired Zero-Shot Learning<br>
[paper](https://arxiv.org/abs/2108.10967)<br><br>

[4] Relational Embedding for Few-Shot Classification<br>
[paper](https://arxiv.org/abs/2108.09666)<br><br>

[3] Boosting the Generalization Capability in Cross-Domain Few-shot Learning via Noise-enhanced Supervised Autoencoder<br>
[paper](https://arxiv.org/abs/2108.05028)<br><br>

[2] Transductive Few-Shot Classification on the Oblique Manifold<br>
[paper](https://arxiv.org/abs/2108.04009)<br><br>

[1] FREE: Feature Refinement for Generalized Zero-Shot Learning<br>
[paper](https://arxiv.org/abs/2107.13807) | [code](https://github.com/shiming-chen/FREE)<br><br>

<br>

<a name="ContinualLearning"/> 

## 持续学习(Continual Learning/Life-long Learning)
 
[4] Lifelong Infinite Mixture Model Based on Knowledge-Driven Dirichlet Process<br>
[paper](https://arxiv.org/abs/2108.12278) | [code](https://github.com/dtuzi123/Lifelong-infinite-mixture-model)<br><br> 

[3] Continual Neural Mapping: Learning An Implicit Scene Representation from Sequential Observations<br>
[paper](https://arxiv.org/abs/2108.05851)<br><br>

[2] RECALL: Replay-based Continual Learning in Semantic Segmentation<br>
[paper](https://arxiv.org/abs/2108.03673)<br><br>

[1] Few-Shot and Continual Learning with Attentive Independent Mechanisms<br>
[paper](https://arxiv.org/abs/2107.14053) | [code](https://github.com/huang50213/AIM-Fewshot-Continual)<br>

<br>

<a name="domain"/> 

## 迁移学习/domain/自适应(Transfer Learning/Domain Adaptation)
 
[16] Learning Cross-modal Contrastive Features for Video Domain Adaptation<br>
[paper](https://arxiv.org/abs/2108.11974)<br><br> 
 
[15] Learning to Diversify for Single Domain Generalization<br>
[paper](https://arxiv.org/abs/2108.11726)<br><br> 

[14] PIT: Position-Invariant Transform for Cross-FoV Domain Adaptation<br>
[paper](https://arxiv.org/abs/2108.07142) | [code](https://github.com/sheepooo/PIT-Position-Invariant-Transform)<br><br>

[13] Multi-Target Adversarial Frameworks for Domain Adaptation in Semantic Segmentation<br>
[paper](https://arxiv.org/abs/2108.06962)<br><br>

[12] Semantic Concentration for Domain Adaptation<br>
[paper](https://arxiv.org/abs/2108.05720)<br><br>

[11] Dual Path Learning for Domain Adaptation of Semantic Segmentation<br>
[paper](https://arxiv.org/abs/2108.06337) | [code](https://github.com/royee182/DPL)<br><br>

[10] Zero-Shot Domain Adaptation with a Physics Prior(Oral)<br>
[paper](https://arxiv.org/abs/2108.05137) | [code](https://github.com/Attila94/CIConv)<br><br>

[9] BiMaL: Bijective Maximum Likelihood Approach to Domain Adaptation in Semantic Scene Segmentation<br>
[paper](https://arxiv.org/abs/2108.03267)<br><br>

[8] Domain Generalization via Gradient Surgery<br><br>
[paper](https://arxiv.org/abs/2108.01621)<br>

[7] Generalizing Gaze Estimation with Outlier-guided Collaborative Adaptation<br>
[paper](https://arxiv.org/abs/2107.13780) | [code](https://github.com/DreamtaleCore/PnP-GA)<br><br>

[6] Adversarial Unsupervised Domain Adaptation with Conditional and Label Shift: Infer, Align and Iterate<br>
[paper](https://arxiv.org/abs/2107.13469)<br><br>

[5] Recursively Conditional Gaussian for Ordinal Unsupervised Domain Adaptation(Oral)<br>
[paper](https://arxiv.org/abs/2107.13467)<br><br>

[4] Improve Unsupervised Pretraining for Few-label Transfer<br>
[paper](https://arxiv.org/abs/2107.12369)<br><br>

[3] Generalized Source-free Domain Adaptation<br>
[homepage](https://sites.google.com/view/g-sfda/g-sfda) | [code](https://github.com/Albert0147/G-SFDA)<br><br>

[2] Seasonal Contrast: Unsupervised Pre-Training from Uncurated Remote Sensing Data(迁移学习)<br>
[paper](https://arxiv.org/abs/2103.16607) ｜ [code](https://github.com/ElementAI/seasonal-contrast)<br><br>

[1] Calibrated prediction in and out-of-domain for state-of-the-art saliency modeling(迁移学习)<br>
[paper](https://arxiv.org/abs/2105.12441)<br><br>

<br>

<a name="MetricLearning"/> 

## 度量学习(Metric Learning)

[6] Deep Relational Metric Learning<br>
[paper](https://arxiv.org/abs/2108.10026) | [code](https://github.com/zbr17/DRML)<br><br>

[5] LoOp: Looking for Optimal Hard Negative Embeddings for Deep Metric Learning<br>
[paper](https://arxiv.org/abs/2108.09335)<br><br>

[4] Towards Interpretable Deep Metric Learning with Structural Matching<br>
[paper](https://arxiv.org/abs/2108.05889) | [code](https://github.com/wl-zhao/DIML)<br><br>

[3] AGKD-BML: Defense Against Adversarial Attack by Attention Guided Knowledge Distillation and Bi-directional Metric Learning<br>
[paper](https://arxiv.org/abs/2108.06017) | [code](https://github.com/hongw579/AGKD-BML)<br><br>

[2] Deep Metric Learning for Open World Semantic Segmentation<br>
[paper](https://arxiv.org/abs/2108.04562)<br><br>

[1] Learning with Memory-based Virtual Classes for Deep Metric Learning<br>
[paper](https://arxiv.org/abs/2103.16940)<br><br>

<br>

<a name="IncrementalLearning"/> 

## 增量学习(Incremental Learning)

[2] Generalized and Incremental Few-Shot Learning by Explicit Learning and Calibration without Forgetting<br>
[paper](https://arxiv.org/abs/2108.08165) | [code](https://github.com/annusha/LCwoF)<br><br>

[1] Always Be Dreaming: A New Approach for Data-Free Class-Incremental Learning<br>
[paper](https://arxiv.org/abs/2106.09701) | [code](https://github.com/GT-RIPL/AlwaysBeDreaming-DFCIL) | [project](https://jamessealesmith.github.io/project/dfcil/)<br><br>

<br>

<a name="ContrastiveLearning"/> 


## 对比学习(Contrastive Learning)

[6] TACo: Token-aware Cascade Contrastive Learning for Video-Text Alignment<br>
[paper](https://arxiv.org/abs/2108.09980)<br>

[5] Self-Supervised Video Representation Learning with Meta-Contrastive Network(对比学习)(元学习)(表征学习)(动作识别)<br>
[paper](https://arxiv.org/abs/2108.08426)<br><br>

[4] Improving Contrastive Learning by Visualizing Feature Transformation<br>
[paper](https://arxiv.org/abs/2108.02982) ｜ [visualization tools and codes](https://github.com/DTennant/CL-Visualizing-Feature-Transformation)<br><br>

[3] Parametric Contrastive Learning<br>
[paper](https://arxiv.org/abs/2107.12028) | [code](https://github.com/jiequancui/Parametric-Contrastive-Learning)<br><br>

[2] Geography-Aware Self-Supervised Learning<br>
[paper](https://arxiv.org/abs/2011.09980)<br><br>

[1] CoMatch: Semi-supervised Learning with Contrastive Graph Regularization<br>
[paper](https://arxiv.org/abs/2011.11183) | [code](https://github.com/salesforce/CoMatch)<br><br>

<br>

<a name="ActiveLearning"/> 

## 主动学习(Active Learning)

[2] Semi-Supervised Active Learning with Temporal Output Discrepancy<br>
[paper](https://arxiv.org/abs/2107.14153) | [code](https://github.com/siyuhuang/TOD)<br><br>

[1] Active Learning for Deep Object Detection via Probabilistic Modeling<br>
[paper](https://arxiv.org/abs/2103.16130)<br><br>

<br>
<a name="VisualReasoning"/> 

## 视觉推理/视觉问答(Visual Reasoning/VQA)

[3] Greedy Gradient Ensemble for Robust Visual Question Answering<br>
[paper](https://arxiv.org/abs/2107.12651) | [code](https://github.com/GeraldHan/GGE)<br><br>

[2] On the hidden treasure of dialog in video question answering<br>
[paper](https://arxiv.org/abs/2103.14517)<br><br>

[1] Just Ask: Learning to Answer Questions from Millions of Narrated Videos(Oral)<br>
[paper](https://arxiv.org/abs/2012.00451) | [code](https://github.com/antoyang/just-ask) | [project](https://antoyang.github.io/just-ask.html)<br><br>

<br>
<a name="MetaLearning"/> 

## 元学习(Meta Learning)

[1] Self-Supervised Video Representation Learning with Meta-Contrastive Network(对比学习)(元学习)(表征学习)(动作识别)<br>
[paper](https://arxiv.org/abs/2108.08426)<br><br>

<br>
<a name="MMLearning"/> 

## 多模态学习(Multi-Modal Learning)

<a name="Audio-VisualLearning"/> 

### 视听学习(Audio-visual Learning)

[1] The Right to Talk: An Audio-Visual Transformer Approach<br>
[paper](https://arxiv.org/abs/2108.03256)<br><br>
 
<a name="VisualLanguage"/>  

### 视觉语言(Visual & Language)

[1] LocTex: Learning Data-Efficient Visual Representations from Localized Textual Supervision<br>
[paper](https://arxiv.org/abs/2108.11950) | [project](https://loctex.mit.edu/)<br><br> 
 
<br>

<a name="Vision-basedPrediction"/> 

## 视觉预测(Vision-based Prediction)

[9] DenseTNT: End-to-end Trajectory Prediction from Dense Goal Sets<br>
[paper](https://arxiv.org/abs/2108.09640)<br><br>

[8] Generating Smooth Pose Sequences for Diverse Human Motion Prediction<br>
[paper](https://arxiv.org/abs/2108.08422) | [code](https://github.com/wei-mao-2019/gsps)<br><br>

[7] MSR-GCN: Multi-Scale Residual Graph Convolution Networks for Human Motion Prediction(人体运动预测)<br>
[paper](https://arxiv.org/abs/2108.07152) ｜ [code](https://github.com/Droliven/MSRGCN)<br><br>

[6] RAIN: Reinforced Hybrid Attention Inference Network for Motion Forecasting(运动预测)<br>
[paper](https://arxiv.org/abs/2108.01316) | [project](https://jiachenli94.github.io/publications/RAIN/)<br><br>

[5] SLAMP: Stochastic Latent Appearance and Motion Prediction(运动预测)<br>
[paper](https://arxiv.org/abs/2108.02760)<br><br>

[4] Unlimited Neighborhood Interaction for Heterogeneous Trajectory Prediction(轨迹预测)<br>
[paper](https://arxiv.org/abs/2108.00238)<br><br>

[3] Personalized Trajectory Prediction via Distribution Discrimination(轨迹预测)<br>
[paper](https://arxiv.org/abs/2107.14204) | [code](https://github.com/CHENGY12/DisDis)<br><br>

[2] Human Trajectory Prediction via Counterfactual Analysis(轨迹预测)<br>
[paper](https://arxiv.org/abs/2107.14202) | [code](https://github.com/CHENGY12/CausalHTP)<br><br>

[1] On Exposing the Challenging Long Tail in Future Prediction of Traffic Actors<br>
[paper](https://arxiv.org/abs/2103.12474)<br><br>

<br>

<a name="Dataset"/> 

## 数据集(Dataset)

[8] From Two to One: A New Scene Text Recognizer with Visual Language Modeling Network<br>
[paper](https://arxiv.org/abs/2108.09661) | [code&dataset](https://github.com/wangyuxin87/VisionLAN)<br><br>

[7] LOKI: Long Term and Key Intentions for Trajectory Prediction(轨迹预测)<br>
[paper](https://arxiv.org/abs/2108.08236) | [dataset](https://usa.honda-ri.com/loki)<br><br>

[6] Who's Waldo? Linking People Across Text and Images(Oral)<br>
[paper](https://arxiv.org/abs/2108.07253) | [project](https://whoswaldo.github.io/)<br><br>

[5] Towards Real-World Prohibited Item Detection: A Large-Scale X-ray Benchmark(违禁物品检测)<br>
[paper](https://arxiv.org/abs/2108.07020)<br><br>

[4] Towers of Babel: Combining Images, Language, and 3D Geometry for Learning Multimodal Vision(地标照片集)<br>
[paper](https://arxiv.org/abs/2108.05863) | [project](https://www.cs.cornell.edu/projects/babel/)<br>

[3] Webly Supervised Fine-Grained Recognition: Benchmark Datasets and An Approach<br>
[paper](https://arxiv.org/abs/2108.02399) | [dataset](https://github.com/NUST-Machine-Intelligence-Laboratory/weblyFG-dataset)<br><br>

[2] OpenForensics: Large-Scale Challenging Dataset For Multi-Face Forgery Detection And Segmentation In-The-Wild<br>
[paper](https://arxiv.org/abs/2107.14480) | [project](https://sites.google.com/view/ltnghia/research/openforensics)<br><br>

[1] 4DComplete: Non-Rigid Motion Estimation Beyond the Observable Surface(4D重建)<br>
[paper](https://arxiv.org/pdf/2105.01905.pdf) | [dataset](https://github.com/rabbityl/DeformingThings4D) | [video](https://youtu.be/QrSsVoTRpWk)<br><br>

<br>

<a name="100"/> 

## 暂无分类
 
SketchLattice: Latticed Representation for Sketch Manipulation<br>
[paper](https://arxiv.org/abs/2108.11636)<br><br>
 
The Surprising Effectiveness of Visual Odometry Techniques for Embodied PointGoal Navigation(视觉导航)<br>
[paper](https://arxiv.org/abs/2108.11550)<br><br>

Learning Signed Distance Field for Multi-view Surface Reconstruction(Oral)<br>
[paper](https://arxiv.org/abs/2108.09964)<br><br>

BlockCopy: High-Resolution Video Processing with Block-Sparse Feature Propagation and Online Policies<br>
[paper](https://arxiv.org/abs/2108.09376)<br><br>

Stochastic Scene-Aware Motion Prediction(运动合成)(运动预测)<br>
[paper](https://arxiv.org/abs/2108.08284) | [project](https://samp.is.tue.mpg.de/)<br><br>

End-to-End Urban Driving by Imitating a Reinforcement Learning Coach(自动驾驶)(强化学习)<br>
[paper](https://arxiv.org/abs/2108.08265)<br><br>

Learning RAW-to-sRGB Mappings with Inaccurately Aligned Supervision<br>
[paper](https://arxiv.org/abs/2108.08119) | [code](https://github.com/cszhilu1998/RAW-to-sRGB)<br><br>

Asymmetric Bilateral Motion Estimation for Video Frame Interpolation(视频插帧)<br>
[paper](https://arxiv.org/abs/2108.06815) | [code](https://github.com/JunHeum/ABME)<br><br>

Focus on the Positives: Self-Supervised Learning for Biodiversity Monitoring<br>
[paper](https://arxiv.org/abs/2108.06435)<br><br>

DiagViB-6: A Diagnostic Benchmark Suite for Vision Models in the Presence of Shortcut and Generalization Opportunities<br>
[paper](https://arxiv.org/abs/2108.05779)<br><br>

MT-ORL: Multi-Task Occlusion Relationship Learning<br>
[paper](https://arxiv.org/abs/2108.05722) | [code](https://github.com/fengpanhe/MT-ORL)<br><br>

ProAI: An Efficient Embedded AI Hardware for Automotive Applications - a Benchmark Study<br>
[paper](https://arxiv.org/abs/2108.05170)<br><br>

Invisible Backdoor Attack with Sample-Speciﬁc Triggers(后门学习)<br>
[paper](https://arxiv.org/abs/2012.03816)<br>
[解读：具有样本特定触发器的隐形后门攻击](https://zhuanlan.zhihu.com/p/394018343)<br><br>

SUNet: Symmetric Undistortion Network for Rolling Shutter Correction<br>
[paper](https://arxiv.org/abs/2108.04775)<br><br>

Learning to Cut by Watching Movies<br>
[paper](https://arxiv.org/abs/2108.04294) | [project](https://alejandropardo.net/publication/learning-to-cut/)<br><br>

Paint Transformer: Feed Forward Neural Painting with Stroke Prediction(Oral)<br>
[paper](https://arxiv.org/abs/2108.03798) | [code](https://github.com/Huage001/PaintTransformer)<br><br>

Internal Video Inpainting by Implicit Long-range Propagation<br>
[paper](https://arxiv.org/abs/2108.01912)<br><br>

CanvasVAE: Learning to Generate Vector Graphic Documents<br>
[paper](https://arxiv.org/abs/2108.01249)<br><br>

TkML-AP: Adversarial Attacks to Top-k Multi-Label Learning(多标签学习)<br>
[paper](https://arxiv.org/abs/2108.00146)<br><br>

Out-of-Core Surface Reconstruction via Global TGV Minimization<br>
[paper](https://arxiv.org/abs/2107.14790)<br><br>

Variational Attention: Propagating Domain-Specific Knowledge for Multi-Domain Learning in Crowd Counting(人群计数)<br>
[paper](https://arxiv.org/abs/2108.08023)<br><br>

Spatial Uncertainty-Aware Semi-Supervised Crowd Counting(人群计数)<br>
[paper](https://arxiv.org/abs/2107.13271)<br><br>

Rethinking Counting and Localization in Crowds: A Purely Point-Based Framework(Oral)(人群计数)<br>
[paper](https://arxiv.org/abs/2107.12746) | [code](https://github.com/TencentYoutuResearch/CrowdCounting-P2PNet)<br><br>

Uniformity in Heterogeneity:Diving Deep into Count Interval Partition for Crowd Counting(人群计数)<br>
[paper](https://arxiv.org/abs/2107.12619) | [code](https://github.com/TencentYoutuResearch/CrowdCounting-UEPNet)<br><br>

Self-Conditioned Probabilistic Learning of Video Rescaling(视频压缩)<br>
[paper](https://arxiv.org/abs/2107.11639)<br><br>

Mixed SIGNals: Sign Language Production via a Mixture of Motion Primitives(手势生成)<br>
[paper](https://arxiv.org/abs/2107.11317)<br><br>

Temporal-wise Attention Spiking Neural Networks for Event Streams Classification<br>
[paper](https://arxiv.org/abs/2107.11711)<br><br>

Click to Move: Controlling Video Generation with Sparse Motion<br>
[paper](https://arxiv.org/abs/2108.08815) | [code](https://github.com/PierfrancescoArdino/C2M)<br><br>

Long-Term Temporally Consistent Unpaired Video Translation from Simulated Surgical 3D Data（视频翻译/医学/视频合成）<br>
[paper](https://arxiv.org/abs/2103.17204)<br><br>

Pathdreamer: A World Model for Indoor Navigation(视觉导航)<br>
[paper](https://arxiv.org/abs/2105.08756)<br><br>

IPOKE: POKING A STILL IMAGE FOR CONTROLLED STOCHASTIC VIDEO SYNTHESIS<br>
[paper](https://arxiv.org/abs/2107.02790) | [code](https://github.com/CompVis/ipoke) | [project](https://compvis.github.io/ipoke/)<br><br>

Putting NeRF on a Diet: Semantically Consistent Few-Shot View Synthesis<br>
[paper](https://arxiv.org/abs/2104.00677) | [project](https://www.ajayj.com/dietnerf)<br><br>

KiloNeRF: Speeding up Neural Radiance Fields with Thousands of Tiny MLPs<br>
[paper](https://arxiv.org/abs/2103.13744) | [code](https://github.com/creiser/kilonerf)<br><br>

<br><br>

<a name="2"/> 

# 2. ICCV2021 Oral（更新中）

[34] Learning Signed Distance Field for Multi-view Surface Reconstruction(Oral)<br>
[paper](https://arxiv.org/abs/2108.09964)<br><br>

[33] PatchMatch-RL: Deep MVS with Pixelwise Depth, Normal, and Visibility(Oral)<br>
[paper](https://arxiv.org/abs/2108.08943) | [code](https://github.com/leejaeyong7/patchmatch-rl)<br><br>

[32] Fine-grained Semantics-aware Representation Enhancement for Self-supervised Monocular Depth Estimation(Oral)<br>
[paper](https://arxiv.org/abs/2108.08829) | [code](https://github.com/hyBlue/FSRE-Depth)<br><br>

[31] PoinTr: Diverse Point Cloud Completion with Geometry-Aware Transformers(点云补全)(Oral)<br>
[paper](https://arxiv.org/abs/2108.08839) | [code](https://github.com/yuxumin/PoinTr)<br><br>

[30] DECA: Deep viewpoint-Equivariant human pose estimation using Capsule Autoencoders(Oral)<br>
[paper](https://arxiv.org/abs/2108.08557) | [code](https://github.com/mmlab-cv/DECA)<br><br>

[29] A Unified Objective for Novel Class Discovery(Oral)<br>
[paper](https://arxiv.org/abs/2108.08536) | [code](https://ncd-uno.github.io/)<br><br>

[28] Multi-Anchor Active Domain Adaptation for Semantic Segmentation(Oral)<br>
[paper](https://arxiv.org/abs/2108.08012)<br><br>

[27] Who's Waldo? Linking People Across Text and Images(Oral)<br>
[paper](https://arxiv.org/abs/2108.07253) | [project](https://whoswaldo.github.io/)<br><br>

[26] LabOR: Labeling Only if Required for Domain Adaptive Semantic Segmentation(Oral)<br>
[paper](https://arxiv.org/abs/2108.05570)<br><br>

[25] Zero-Shot Domain Adaptation with a Physics Prior(Oral)<br>
[paper](https://arxiv.org/abs/2108.05137) | [code](https://github.com/Attila94/CIConv)<br><br>

[24] An Empirical Study of Training Self-Supervised Vision Transformers(Oral)<br>
[paper](https://arxiv.org/abs/2104.02057)<br>
[解读：解决训练不稳定性，何恺明团队新作来了！自监督学习+Transformer=MoCoV3](https://mp.weixin.qq.com/s/L4a6ZHTPkzQPS5VVm6Hm-A)<br><br>

[23] Paint Transformer: Feed Forward Neural Painting with Stroke Prediction(Oral)<br>
[paper](https://arxiv.org/abs/2108.03798) | [code](https://github.com/Huage001/PaintTransformer)<br><br>

[22] (Just) A Spoonful of Refinements Helps the Registration Error Go Down(Oral)<br>
[paper](https://arxiv.org/abs/2108.03257)<br><br>

[21] ILVR: Conditioning Method for Denoising Diffusion Probabilistic Models(Oral)<br>
[paper](https://arxiv.org/abs/2108.02938)<br><br>

[20] ACE: Ally Complementary Experts for Solving Long-Tailed Recognition in One-Shot(Oral)<br>
[paper](https://arxiv.org/abs/2108.02385) | [code](https://github.com/jrcai/ACE)<br><br>

[19] An Intermediate Domain Module for Domain Adaptive Person Re-ID(Oral)<br>
[paper](https://arxiv.org/abs/2108.02413) | [code](https://github.com/SikaStar/IDM)<br><br>

[18] Recursively Conditional Gaussian for Ordinal Unsupervised Domain Adaptation(Oral)<br>
[paper](https://arxiv.org/abs/2107.13467)<br><br>

[17] Rethinking Counting and Localization in Crowds: A Purely Point-Based Framework(Oral)(人群计数)<br>
[paper](https://arxiv.org/abs/2107.12746) | [code](https://github.com/TencentYoutuResearch/CrowdCounting-P2PNet)<br><br>

[16] Rank & Sort Loss for Object Detection and Instance Segmentation(Oral)<br>
[paper](https://arxiv.org/abs/2107.11669) | [code](https://github.com/kemaloksuz/RankSortLoss)<br>
[解读：拒绝调参，显著提点！检测分割任务的新损失函数RS Loss开源](https://mp.weixin.qq.com/s/OPw4uGc_VFtHt98SgC1yJg)<br><br>

[15] Transporting Causal Mechanisms for Unsupervised Domain Adaptation<br>
[paper](https://arxiv.org/abs/2107.11055)<br><br>

[14] Standardized Max Logits: A Simple yet Effective Approach for Identifying Unexpected Road Obstacles in Urban-Scene Segmentation(Oral)<br>
[paper](https://arxiv.org/abs/2107.11264<br><br>

[13] Re-distributing Biased Pseudo Labels for Semi-supervised Semantic Segmentation: A Baseline Investigation(Oral)<br>
[paper](https://arxiv.org/abs/2107.11279) | [code](https://github.com/CVMI-Lab/DARS)<br><br>

[12] Human Pose Regression with Residual Log-likelihood Estimation(Oral)<br>
[paper](https://arxiv.org/abs/2107.11291) | [code](https://github.com/Jeff-sjtu/res-loglikelihood-regression)<br><br>

[11] Robustness via Cross-Domain Ensembles(Oral)<br>
[paper](https://crossdomain-ensembles.epfl.ch/#paper) | [code](https://github.com/EPFL-VILAB/XDEnsembles) | [model](https://github.com/EPFL-VILAB/XDEnsembles#pretrained-models) | [homepage](https://crossdomain-ensembles.epfl.ch/)<br><br>

[10] Warp Consistency for Unsupervised Learning of Dense Correspondences(Oral)<br>
[paper](https://arxiv.org/abs/2104.03308) | [code](https://github.com/PruneTruong/DenseMatching)<br><br>

[9] PyMAF: 3D Human Pose and Shape Regression with Pyramidal Mesh Alignment Feedback Loop(Oral)<br>
[paper](https://arxiv.org/pdf/2103.16507.pdf) | [code](https://github.com/HongwenZhang/PyMAF) | [project](https://hongwenzhang.github.io/pymaf)<br><br>

[8] HuMoR: 3D Human Motion Model for Robust Pose Estimation(Oral)<br>
[paper](https://geometry.stanford.edu/projects/humor/docs/humor.pdf) | [video](https://youtu.be/5VWirxUHG0Y) | [project](https://geometry.stanford.edu/projects/humor/)<br><br>

[7] Generic Attention-model Explainability for Interpreting Bi-Modal and Encoder-Decoder Transformers(Oral)<br>
[paper](https://arxiv.org/pdf/2103.15679.pdf) | [code](https://github.com/hila-chefer/Transformer-MM-Explainability)<br><br>

[6] Equivariant Imaging: Learning Beyond the Range Space(Oral)<br>
[paper](https://arxiv.org/pdf/2103.14756.pdf)<br><br>

[5] MDETR : Modulated Detection for End-to-End Multi-Modal Understanding(Oral)<br>
[paper](https://arxiv.org/pdf/2104.12763) | [code](https://github.com/ashkamath/mdetr) | [project](https://ashkamath.github.io/mdetr_page/) | [colab](https://colab.research.google.com/github/ashkamath/mdetr/blob/colab/notebooks/MDETR_demo.ipynb)<br>
[解读：无需检测器提取特征！LeCun团队提出MDETR：实现真正的端到端多模态推理](https://zhuanlan.zhihu.com/p/394194465)<br><br>

[4] Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions(Oral)<br>
[paper](https://arxiv.org/abs/2102.12122) | [code](https://github.com/whai362/PVT)<br>
[解读：金字塔视觉Transformer(PVT)：用于密集预测的多功能backbone](https://mp.weixin.qq.com/s/Ruqgm4lMcXveJkJUVCHrLg)<br><br>

[3] Mining Latent Classes for Few-shot Segmentation(Oral)<br>
[paper](https://arxiv.org/abs/2103.15402) | [code](https://github.com/LiheYoung/MiningFSS)<br><br>

[2] In-Place Scene Labelling and Understanding with Implicit Scene Representation(Oral)<br>
[paper](https://arxiv.org/abs/2103.15875) | [project](https://shuaifengzhi.com/Semantic-NeRF/)<br><br>

[1] Just Ask: Learning to Answer Questions from Millions of Narrated Videos(Oral)<br>
[paper](https://arxiv.org/abs/2012.00451) | [code](https://github.com/antoyang/just-ask)<br><br>

<br><br>

<a name="3"/> 

# 3. ICCV2021论文解读汇总（更新中）

[16] An Empirical Study of Training Self-Supervised Vision Transformers(Oral)<br>
[paper](https://arxiv.org/abs/2104.02057)<br>
[解读：解决训练不稳定性，何恺明团队新作来了！自监督学习+Transformer=MoCoV3](https://mp.weixin.qq.com/s/L4a6ZHTPkzQPS5VVm6Hm-A)<br><br>

[15] Bias Loss for Mobile Neural Networks<br>
[paper](https://arxiv.org/abs/2107.11170)<br>
[解读：超越MobileNet V3 | 详解SkipNet+Bias Loss=轻量化模型新的里程碑](https://mp.weixin.qq.com/s/BSCSGdQrQ1F1-5ofuH5lTA)<br><br>

[14] Rethinking and Improving Relative Position Encoding for Vision Transformer<br>
[paper](https://arxiv.org/abs/2107.14222) | [code](https://github.com/microsoft/Cream/tree/main/iRPE)<br>
[解读：Vision Transformer中的相对位置编码](https://mp.weixin.qq.com/s/xwqzVb696GbXDxCVN0-cGA)<br><br>

[13] Spatial-Temporal Transformer for Dynamic Scene Graph Generation<br>
[paper](https://arxiv.org/abs/2107.12309)<br>
[解读：用于视频场景图生成的时空上下文Transformer](https://zhuanlan.zhihu.com/p/393637591)<br><br>

[12] LeViT: a Vision Transformer in ConvNet’s Clothing for Faster Inference<br>
[paper](https://arxiv.org/abs/2104.01136) | [code](https://github.com/facebookresearch/LeViT)<br>
[解读：FaceBook提出LeViT，0.077ms的单图处理速度却拥有ResNet50的精度](https://mp.weixin.qq.com/s/LtVANU9wHdzK_mNUO47yLA)<br><br>

[11] Progressive Correspondence Pruning by Consensus Learning<br>
[paper](https://arxiv.org/abs/2101.00591) | [code](https://github.com/sailor-z/CLNet) | [project](https://sailor-z.github.io/projects/CLNet.html)<br>
[解读：CLNet：基于一致性学习的渐进式匹配筛选](https://zhuanlan.zhihu.com/p/394483122)<br><br>

[10] Invisible Backdoor Attack with Sample-Speciﬁc Triggers(后门学习)<br>
[paper](https://arxiv.org/abs/2012.03816)<br>
[解读：具有样本特定触发器的隐形后门攻击](https://zhuanlan.zhihu.com/p/394018343)<br><br>

[9] GraphFPN: Graph Feature Pyramid Network for Object Detection<br>
[paper](https://arxiv.org/abs/2108.00580)<br>
[解读：复旦&港大提出GraphFPN：用图特征金字塔提升目标检测性能！](https://mp.weixin.qq.com/s/jzyXrNTEiT6jrvXYszkELQ)<br><br>

[8] Learning Spatio-Temporal Transformer for Visual Tracking<br>
[paper](https://arxiv.org/abs/2103.17154) | [code](https://github.com/researchmm/Stark)<br>
[解读：屠榜目标跟踪！大连理工和MSRA提出STARK：基于Transformer的目标跟踪器](https://mp.weixin.qq.com/s/Tlrhoj2jnexu9mjRky0yww)<br><br>

[7] Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet<br>
[paper](https://arxiv.org/abs/2101.11986) | [code](https://github.com/yitu-opensource/T2T-ViT)<br>
[解读：ResNet被全面超越了，是Transformer干的：依图科技开源“可大可小”T2T-ViT，轻量版优于MobileNet](https://mp.weixin.qq.com/s/MjsarhMgKv4dvHJysdWFcA)<br><br>

[6] Sketch Your Own GAN<br>
[paper](https://arxiv.org/abs/2108.02774) | [code](https://peterwang512.github.io/) | [project](https://peterwang512.github.io/GANSketching)<br>
[解读：用一张草图创建GAN模型，新手也能玩转，朱俊彦团队新研究入选ICCV 2021](https://mp.weixin.qq.com/s/pXsMJEhHRdYFYOqd4oYiCQ)<br><br>

[5] DetCo: Unsupervised Contrastive Learning for Object Detection<br>
[paper](https://arxiv.org/abs/2102.04803) | [code](https://github.com/xieenze/DetCo)<br>
[解读：性能优于何恺明团队MoCo v2，DetCo：为目标检测定制任务的对比学习](https://zhuanlan.zhihu.com/p/393163169)<br><br>

[4] MDETR : Modulated Detection for End-to-End Multi-Modal Understanding(Oral)<br>
[paper](https://arxiv.org/pdf/2104.12763) | [code](https://github.com/ashkamath/mdetr) | [project](https://ashkamath.github.io/mdetr_page/) | [colab](https://colab.research.google.com/github/ashkamath/mdetr/blob/colab/notebooks/MDETR_demo.ipynb)<br>
[解读：无需检测器提取特征！LeCun团队提出MDETR：实现真正的端到端多模态推理](https://zhuanlan.zhihu.com/p/394194465)<br><br>

[3] MixMo: Mixing Multiple Inputs for Multiple Outputs via Deep Subnetworks<br>
[paper](https://arxiv.org/abs/2103.06132)<br>
[解读：“白嫖”性能的MixMo，一种新的数据增强or模型融合方法](https://zhuanlan.zhihu.com/p/396554361)<br><br>

[2] TransReID: Transformer-based Object Re-Identification<br>
[paper](https://arxiv.org/abs/2102.04378) | [code](https://github.com/heshuting555/TransReID)<br>
[解读：来自Transformer的降维打击：ReID各项任务全面领先，阿里&浙大提出TransReID](https://mp.weixin.qq.com/s/rATLyYBgo2nWY4rKXmgV5w)<br><br>

[1] Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions(Oral)<br>
[paper](https://arxiv.org/abs/2102.12122) | [code](https://github.com/whai362/PVT)<br>
[解读：金字塔视觉Transformer(PVT)：用于密集预测的多功能backbone](https://mp.weixin.qq.com/s/Ruqgm4lMcXveJkJUVCHrLg)<br><br>
